{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiJWOWCRDB-F"
      },
      "source": [
        "# Background\n",
        "\n",
        "In this case study, we will develop a model to predict spatial variations in **annual average** **outdoor ultrafine particle (UFP) concentrations** across the Bucaramanga metropolitan area in Colombia. We are reproducing work that was published here: https://pubs.acs.org/doi/10.1021/acs.est.1c01412\n",
        "  \n",
        "**Setting**  \n",
        "The Bucaramanga metropolitan area consists of the following cities: Bucaramanga, Floridablanca, Giron, and Piedecuesta. It has a population of approximately 1.3 million and has been growing rapidly in recent years. There are some databases available with limited land use information, but not nearly as much as we would find for a large city in a high income country. That makes developing a land use regression model a challenge, but it is an opportunity to use alternative data streams such as digital images to train deep convolutional neural network models. We will train our models on monitoring data and either satellite or street-view images.    \n",
        "\n",
        "**Monitoring Data**  \n",
        "During the monitoring campaing, UFP concentrations were repeatedly measured at 1-second intervals along roads throughout study area during a 2-week period. The road network was divided into 100 m long road segments and the median of all UFP measurements along a given road segment was assigned to the centroid (i.e. middle) of the road segment as the estimate of the annual average. The number of UFP measurements along a given road segment is reffered to as the \"join_count\" in this data (i.e., how many 1-second points were joined to the centroid of the road segment). The mean meteorological conditions at the nearest airport during monitoring were measured and linked to the aggregated monitoring data. This was done to account for weather-related temporal variations in UFP concentrations during monitoring (additional details below).  \n",
        "Black Carbon (BC) concentrations were also measured during the monitoring campaign and those data have been included in case you would like to try modelling BC spatial variations on your own.\n",
        "  \n",
        "**Satellite Images**  \n",
        "Two satellite-view images were downloaded for each road segment using the *googleway* package in R. The images were centered on each road segment and one was at a zoom level of 18 and the other was at a zoom level of 19. Picking the zoom level is a bit of an art. A smaller zoom level will have a lot of overlap between images (e.g., zoom 0 is the whole world and all the images would be the same!) A large zoom level would lose some of the contextual information around the road segment. You can play around in google maps to get a feel for the spatial coverage and trade-offs between zoom levels.\n",
        "\n",
        "**Street Images**  \n",
        "Two street-view images were downloaded for each road segment using the *googleway* package as well. The images were oriented to look up and down the street.\n",
        "  \n",
        "**Temporal Variations**\n",
        "This mobile monitoring campiagn used a single monitor in a vehicle that drove around the study area. Roads were visited multiple times in an effort to get a a sample of measurements that representated the annual average, but the road segments were still visited a different times of day with different weather conditions. To account for this, we will later include meteorological conditions during monitoring with the CNN predictions in a regression in order to help control for the weather-related temporal variations in the monitoring data. This will be done after training the CNN models.\n",
        "\n",
        "Additional background information can be found in the published article: https://pubs.acs.org/doi/10.1021/acs.est.1c01412 (same link as above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sndAvCYLpkPC"
      },
      "source": [
        "# Set Runtime (IMPORTANT!)\n",
        "\n",
        "Goolge Colab offers free acess to hardware upgrades such as a Graphics Processing Unit (GPU). That will allow us to train our deep learning models in a reasonable amount of time. If you try to train using a CPU, it will take a very ***very*** long time. Here's a quick blog post explaining why: https://towardsdatascience.com/what-is-a-gpu-and-do-you-need-one-in-deep-learning-718b9597aa0d\n",
        "\n",
        "\n",
        "***Change to a GPU using the drop down menu***\n",
        "> Runtime > Change runtime type > GPU > OK\n",
        "\n",
        "This will restart your runtime, which removes any data and variables that you have read in. If you don't change to a GPU here, then you would end up losing all the prepared data and have to redo it.\n",
        "\n",
        "After you have changed the runtime, run the code below to verify that you are indeed connected to a GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U024fYivJktz",
        "outputId": "ad3ad49a-5351-4de9-b33b-ca933bef6299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good to go! Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "# this code checks to see if you have a GPU available\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found, make sure to connect to GPU') # if not avaible, you will get an error\n",
        "print('Good to go! Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkN-bluFu14q"
      },
      "source": [
        "# FYI - Some Google Colab Information and Tips\n",
        "\n",
        "We've shared a workbook called \"Python and Colab Intro\" that can help show you some basics. Additionally:  \n",
        "\n",
        "Google Colab FAQs\n",
        "https://research.google.com/colaboratory/faq.html#:~:text=Runtimes%20will%20time%20out%20if,on%20your%20compute%20unit%20balance.\n",
        "\n",
        "Google Colab tips:  \n",
        "https://www.analyticsvidhya.com/blog/2021/05/10-colab-tips-and-hacks-for-efficient-use-of-it/\n",
        "\n",
        "There are keyboard shortcuts/commands that can be useful, but it can take a bit of getting used to. If you are new to Google Colab, don't bother with using them just yet. If you do want to try, push \"Ctrl+M\" (\"⌘+M\" on Mac), then release those keys and push \"H\" to show the keyboard shortcuts.  \n",
        "Some useful commands:\n",
        ">  \"⌘/Ctrl+M then H\": show keyboard shortcuts (this also allows you to set them)  \n",
        ">  \"⌘/Ctrl+M then -\": splits cell where the cursor is  \n",
        ">  \"⌘/Ctrl+M then B\": insert code cell below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqN8LNe7upta"
      },
      "source": [
        "# Importing Modules/Packages\n",
        "\n",
        "Like in R, there are several packages that we want to load/import. If you want to be pedantic, some are technically \"modules\", some are \"submodules\" and others are \"packages\". For our purposes it doesn't matter and we'll just call them all packages here.  \n",
        "\n",
        "Time to load our packages. We are also going to define Root Mean Squared Error (RMSE). We will need that later when we are training models on continuous values of UFP concentrations. The Keras package has Mean Absolute Error (MAE), but not RMSE. We prefer using RMSE as the loss function because it gives a greater penalty to large errors than MAE.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PFm5TrYmpvV"
      },
      "outputs": [],
      "source": [
        "#########################################################################################################\n",
        "##################### Run this cell whenever you connect to a new runtime ###############################\n",
        "#########################################################################################################\n",
        "\n",
        "# this step loads the packages we will be using\n",
        "\n",
        "# deep learning and data science packages\n",
        "import tensorflow as tf # if your instance of google colab can't load tensorflow, close and re-open\n",
        "from tensorflow import keras as K\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# for file processing and moving\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# for figures\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "\n",
        "# for linear regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# for the Grad-CAMs\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# define root mean square error (rmse) for use as the loss function when training models on continuous UFP. mae is also available\n",
        "def rmse(y_true, y_pred):\n",
        "    return K.backend.sqrt(K.backend.mean(K.backend.square(y_pred - y_true)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEr6Z-tcExtA"
      },
      "source": [
        "# Loading Data and Setting Up Folders\n",
        "\n",
        "There are several methods you can use to access the data in your Google Drive.\n",
        "\n",
        "> ***We will run code to mount entire drive, then read in files***\n",
        "\n",
        "Colab disconnect your runtime if you sit idle too long. When you reconnect, you have to re-read in the data. For that reason, we like to mount the drive using code. It is also possible to mount the drive by pointing and clicking on the folder in the tab to the left and then clicking the Google Drive icon (folder with triangle on it).  \n",
        "\n",
        "Run the code below to mount the drive. You will get a pop up that will then open a new tab. Agree to both.   \n",
        "\n",
        "> ***This can take about 10 - 60 seconds to run***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR09GWxeSuCy",
        "outputId": "4bc5cbc8-a8af-4913-879a-9c0018b840af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#########################################################################################################\n",
        "##################### Run this cell whenever you connect to a new runtime ###############################\n",
        "#########################################################################################################\n",
        "\n",
        "# use this code to mount your drive. it may take a moment.\n",
        "# a pop up and new tab will ask for permissions to read and write in your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# # in case you want to unmount the drive using code\n",
        "# drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsJdnVLqHNAE"
      },
      "source": [
        "After giving persmission, google drive will show up in the folders. You can click on the folder icon in the ribbon to the left to show/hide the folders. Your Google drive should be in \"*/drive*\" and \"*/MyDrive*\". Within \"*/MyDrive*\", you should be able to find the \"*/sharp_ml_course*\" folder that we shared with you.\n",
        "\n",
        "> *If you can't find the \"/sharp_ml_course\" in the folders in the ribbon to the left, then you likely need to add the Google Drive folder we shared with you. Go to your Google Drive (drive.google.com), sign in, click on \"Shared with me\" on the left, find and right click on \"sharp_ml_course\", click on \"Add shortcut to Drive\", make sure \"My Drive\" is highlighted and then click \"ADD SHORTCUT\".*\n",
        "\n",
        "FYI - For the \"point and click\" method to mounting the drive, you click on the folder in the ribbon to show folders, then you click on the folder with the triangle (i.e., google drive symbol) and then click the refresh folder icon.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ7WJFtpIms_"
      },
      "source": [
        "**Unzip Images**\n",
        "\n",
        "We'll look at the metadata in a moment, but first we should unzip the images into a working folder. We'll start by creating a folder called \"images\" to unzip the images into. This will just be a working folder for this session. If the runtime restarts, then we'll have to unzip again, but that's actually kind of handy. It's a pain to have tons of unzipped images in a folder - slow to navigate through, slow to copy, slow to move. It's just easier to unzip them with code when we need them.\n",
        "\n",
        "\n",
        "> ***Unzipping all the images will take about 30 - 60 seconds***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wAd_X6RQ-lZ"
      },
      "outputs": [],
      "source": [
        "#########################################################################################################\n",
        "##################### Run this cell whenever you connect to a new runtime ###############################\n",
        "#########################################################################################################\n",
        "\n",
        "# if the directory (i.e., folder) doesn't already exist, make the folder\n",
        "if not os.path.exists('/content/images'):\n",
        "  !mkdir /content/images\n",
        "\n",
        "# this will take about 30 - 60 seconds\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/sharp_ml_course/colombia_sat_street_images/colombia_sat_street_images.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content/images/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhbu6Fuj5Dnp"
      },
      "source": [
        "\n",
        "Once this is done you will see the subfolders. Take a moment to see how the folders are organized. In this session, you should avoid expanding the images subfolders (e.g., images_18); they have thousands of images and it's slow to show all of them.\n",
        "> images\n",
        ">> images_18 *(zoom 18 satellite images)*  \n",
        ">> images_19 *(zoom 19 satellite images)*  \n",
        ">> images_angle1 *(street images looking either up or down the street)*  \n",
        ">> images_angle2 *(street images looking in the other direction)*  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlyD-YVJK6aF"
      },
      "source": [
        "**Read in Metadata**\n",
        "\n",
        "In the folder ribbon on the left, go to the \"*/sharp_ml_course*\" folder. You can right click on the \"*colombia_image_ghp7_metadata.csv*\" file to copy the file path. The filepath in the code cell below should be correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO6sU9YdGxhm"
      },
      "outputs": [],
      "source": [
        "# read in the metadata. if this code doesn't work, verify the path to the csv file\n",
        "metadata = pd.read_csv(\"/content/drive/MyDrive/sharp_ml_course/colombia_image_ghp7_metadata_sharp2023.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNYsJiNmKPzF"
      },
      "source": [
        "\n",
        "# Directories (Folders) for Outputs\n",
        "\n",
        "For our purposes here, we use the terms \"directories\" and \"folders\" interchangably.  \n",
        "\n",
        "Let's create some folders to save our outputs. In this case, we want to save them to our Colab Notebooks folder in the google drive. It takes a long time to train these models, so we want to make sure we keep them even if the session restarts. If we saved them in */content* then we would lose them when we disconnected from our runtime.   \n",
        "\n",
        "Notice that we have to make parent directories first. You'll get an error if you try to create a directories that already exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npJwXU2bRSfu"
      },
      "outputs": [],
      "source": [
        "# if the directories don't exist, make them. only evaluating the top directory\n",
        "if not os.path.exists('/content/drive/MyDrive/colab_files'):\n",
        "  !mkdir /content/drive/MyDrive/colab_files\n",
        "if not os.path.exists('/content/drive/MyDrive/colab_files/sharp_d2_UFP'):\n",
        "  !mkdir /content/drive/MyDrive/colab_files/sharp_d2_UFP\n",
        "  !mkdir /content/drive/MyDrive/colab_files/sharp_d2_UFP/output\n",
        "  !mkdir /content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model_log\n",
        "  !mkdir /content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model\n",
        "  !mkdir /content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model_predictions\n",
        "  !mkdir /content/drive/MyDrive/colab_files/sharp_d2_UFP/output/grad_cam_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOKtrzClNfRi"
      },
      "source": [
        "# Explore the Metadata\n",
        "\n",
        "Now let's take a look at the metadata. This will show the monitoring data and file names of the associated images. We'll explain the variables further down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shbL7vtyNdG1"
      },
      "outputs": [],
      "source": [
        "# this will print out the metadata variable\n",
        "metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2N-VCo-S-HI"
      },
      "source": [
        "\n",
        "\n",
        "**Metadata Columns**  \n",
        "*id*: id number we have given to road segments  \n",
        "*join_count*: mount of monitoring time (seconds) on a given road segment  \n",
        "*long*: longitude, WSG 84  \n",
        "*lat*: latitude, WSG 84  \n",
        "*road_angle*: road angle was used to download street view images to make sure they were looking up and down the street  \n",
        "*geohash*: this indicates what gridsquare the road segment is in, we use this to split the data in order to minimize the overlap between training and test sets  \n",
        "*file*: name of the image file, notice how the number is the same as the id  \n",
        "*ufp*: ultrafine particle concentration (pt/cm3)  \n",
        "*ufp_cat*: quitile of UFP concentration   \n",
        "*bc*: black carbon concentration (ng/m3)  \n",
        "*bc_cat*: quitile of BC concentration  \n",
        "*temp_f*: mean tempreature during monitoring (deg F)  \n",
        "*relhum_pct*: mean relative humidity during monitoring (%)  \n",
        "*winddir_deg*: mean wind direction during monitoring (degrees)  \n",
        "*windspeed_kts*: mean windspeed during monitoring (knots)  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW8tLQNGTSNO"
      },
      "source": [
        "**Strings vs Numeric**  \n",
        "\n",
        "As humans, we know that *3*, *three*, and *\\\"3\"* all mean the same thing. Python is not human and will treat those differently. The code below shows you the data type of each column in the metadata. Note that the dtype for columns of strings is \"object\" (the pandas module does this for efficiency reasons).  \n",
        "\n",
        "When we read in the metadata, Python recognized that the *ufp_cat* column was a bunch of numbers, specifically integers. That's great if we want to treat the UFP quintiles as numeric values, but if we want to treat it as categories, we will have to change it to string. It's as though we are changing *3* to *three*.  \n",
        "\n",
        "Using the wrong data type is an easy way to get tripped up. One of the first things you should do when troubleshooting code is check all the variable data types and make sure they are appropriate.   \n",
        "\n",
        "More reading on data types:  \n",
        "https://www.geeksforgeeks.org/python-data-types/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNXS8Eyt1TMl"
      },
      "outputs": [],
      "source": [
        "# .dtypes at the end of a variable will print out the data types for each column\n",
        "metadata.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX3_Ab8gObJF"
      },
      "source": [
        "\n",
        "**Exploratory Plots**\n",
        "\n",
        "Recall that *join_count* is the amount of monitoring time on a given road segment. The histogram below shows the distribution of monitoring time per road segment.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1UA1f41OV6D"
      },
      "outputs": [],
      "source": [
        "# using 'matplotlib' package (plt.),  plot a histogram of 'join_cout'\n",
        "plt.hist(x= metadata['join_count'], bins='auto', color='#0504aa', rwidth=0.85) # rwidth=0.85 adds a bit of space between columns\n",
        "plt.xlabel('Road Segement Monitoring Time (s)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Road Segment Monitoring Times')\n",
        "plt.xlim(xmin = -1, xmax = 200)  # limit to 200 so it is easier to visualize\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYPKqZd_YISS"
      },
      "source": [
        "\n",
        "We probably don't want to keep road segments that only have 1 second of monitoring time (does 1 second of monitoring represent the annual average?!?), but we also don't want to be too strict and lose too much spatial coverage. If we insist on a minimum of 120 seconds per road segment, we'll have much less spatial coverage. Run the code below to see the spatial coverage when restricting to minimum 120 seconds vs 10 seconds per road segment. You can check out the Bucaramanga metropolitan area in Google Maps for reference:  \n",
        "https://goo.gl/maps/oKmdRzAZjDp8fhKw8\n",
        "\n",
        "\n",
        "In the end, we decided to use 10 seconds as our threshold. You could probably justify using other thresholds as well (e.g., 20 seconds). You can take a look at the supplementary material of the published article and see our results when using different thresholds.\n",
        "\n",
        "***Try later:*** *You can try developing models using different thresholds. What happens to model R2? What happens to spatial coverage?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZgvaDcdU6mO"
      },
      "outputs": [],
      "source": [
        "# to create two plots side by side, we define the figure and the two subplots\n",
        "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(10, 7))\n",
        "\n",
        "# then we fill the subplots (notice ax = ax1).\n",
        "# Here we are using the 'seaborn' (sns.) package for plots because I like the defaults for scatter plots\n",
        "sns.scatterplot(x='long', y='lat', data=metadata.loc[metadata['join_count']> 120], s = 10, ax = ax1).set(title='Road Segments with \\n>120s of Monitoring')\n",
        "sns.scatterplot(x='long', y='lat', data=metadata.loc[metadata['join_count']> 10], s = 10, ax = ax2).set(title='Road Segments with \\n>10s of Monitoring')\n",
        "\n",
        "# note that these are just scatter plots of longitude and latitute. they are somewhat distorted because they are not true spatial plots. this is fine for exploratory purposes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duYArStfKSnF"
      },
      "source": [
        "Let's also look at histograms of the measured UFP and BC concentrations. This is useful because if the distribution is heavily skewed, we may chose to log-transform it. We can see from the plots below that only BC is skewed.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytXbUSl852Hm"
      },
      "outputs": [],
      "source": [
        "# here is a different approach to plotting figures side by side.\n",
        "f = plt.figure(figsize=(12,5)) # set figure size\n",
        "ax = f.add_subplot(121) # sets up first plot, (121) sets up the position https://stackoverflow.com/questions/3584805/what-does-the-argument-mean-in-fig-add-subplot111\n",
        "ax2 = f.add_subplot(122) # sets up second plot\n",
        "\n",
        "# fill in the two plots with histograms\n",
        "ax.hist(x= metadata.loc[metadata['join_count'] >= 10, 'ufp'], bins='auto', color='purple',rwidth=0.85)\n",
        "ax2.hist(x= metadata.loc[metadata['join_count'] >= 10, 'bc'], bins='auto', color='grey', rwidth=0.85)\n",
        "\n",
        "# set labels, titles, and limits for each plot\n",
        "ax.set_xlabel('UFP Concentration (pt/cm3)') # upadate label to quint vs continous\n",
        "ax2.set_xlabel('BC Concentration (ng/m3)')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_title('Observed UFP Concentrations (min 10 s/road segment)')\n",
        "ax2.set_title('Observed BC Concentrations (min 10 s/road segment)')\n",
        "ax2.set_xlim(0, 120000) # set the axis limits on the BC plot to help with the visual. I did this knowing what it looked like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T7OI3on8RgX"
      },
      "source": [
        "**Geohash Codes**  \n",
        "\n",
        "A geohash code is a unique alpha-numeric code for gridsquares anywhere on earth. You can think of it as a way to spatially group our road segments together. This will be useful for when we split our data into train, validate, and test sets. The quick scatter plot below shows a section of the study area and the colors represent different geohash gridsquares. Notice how the colors are grouped together.         \n",
        "\n",
        "More information on geohash codes can be found here:  \n",
        "http://ellse.org/uncategorized/how-geohash-works/  \n",
        "\n",
        "https://www.movable-type.co.uk/scripts/geohash.html  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhlF-Lb9TQ_6"
      },
      "outputs": [],
      "source": [
        "# make a subset of the data that covers a small area. it would be cluttered and confusing if we plotted all the data\n",
        "# data.loc[(condition) & (condition)] is used to filter by multiple conditions\n",
        "sub_metadata = metadata.loc[(metadata['lat'] < 7.163) & (metadata['lat']> 7.155) & (metadata['long'] < -73.128) & (metadata['long'] > -73.138)]\n",
        "\n",
        "# use seaborn to create a scatter plot\n",
        "sns.scatterplot(x='long', y='lat', data= sub_metadata, hue='geohash', legend = False)\n",
        "# note that this is just a scatter plot of longitude and latitute. it is distorted because it is not a true spatial plot. this is fine for exploratory purposes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBQhT0leQy5C"
      },
      "source": [
        "**Instrument Limit of Detection**\n",
        "\n",
        "The BC monitor was a microAeth MA200, which has a lower limit of detection of 30 ng/m3.\n",
        "\n",
        "The UFP monitor was a TSI CPC 3007, which has a lower limit of detection of 0 pt/cm3. It also has an upper limit of detection of 100,000 pt/cm3. Measurements over this limit are less accurate.  \n",
        "\n",
        "We will impute observations that fall below the lower limit of detection with half the value of the lower limit of detection. We won't change any of the values above the upper limit of detect, but it's good to keep in mind that this is a limitation of the data we are working with.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ziDs_caRyd5"
      },
      "outputs": [],
      "source": [
        "# inspect to see if there are any rows with BC less than 30 or UFP less than zero\n",
        "# | is \"or\"\n",
        "metadata.loc[(metadata['bc'] < 30) | (metadata['ufp'] < 0)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yg1_SrlMLZs"
      },
      "source": [
        "# Preparing the Metadata\n",
        "\n",
        "\n",
        "Now we'll setup the metadata:  \n",
        "1. Restrict the data to road segments with at least 10 seconds of monitoring;  \n",
        "2. Impute any BC observations that are below the lower limit of detection (recall that there were no UFP observations below the limit of detection);\n",
        "3. Log-transform the continuous BC column;  \n",
        "4. Create categorical columns for the quintiles that have \"string\" values instead of numeric values. We'll keep the numeric value quintile columns in case we want to use them later;  \n",
        "5. Split the data into train, test, and validate set. We'll split randomly by geohash code. We split by geohash code in order to spatially separate the sets, which will help reduce the overlap of the images in the sets. This makes them more independent and reduces \"cheating\".\n",
        "\n",
        "\n",
        "*Try later: After this workshop, you could try splitting the data completely randomly (i.e., by \"id\" instead of \"geohash\") to see if it changes the model performance in the test set. Does a little cheating results in higher performance?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hstCBOMKuIOD"
      },
      "outputs": [],
      "source": [
        "# we'll read the metadata in again. this ensures that we are always cleaning the raw data\n",
        "# this also ensures that if we accidentally run this code twice, we'll still set up the metadata correctly\n",
        "metadata = pd.read_csv(\"/content/drive/MyDrive/sharp_ml_course/colombia_image_ghp7_metadata_sharp2023.csv\") # you could comment out this line of code and run this cell twice to see what happens\n",
        "\n",
        "# restrict to road segements with at least 10 seconds of monitoring\n",
        "metadata = metadata.loc[metadata['join_count'] >= 10] # you can replace 10 with a different value if you want to tray developing a model using a different threshold\n",
        "\n",
        "# impute any BC values that are below the lower limit of detection (30) with half that value (15)\n",
        "metadata.loc[metadata['bc'] < 30, 'bc'] = 15\n",
        "\n",
        "# log transform bc\n",
        "metadata['bc'] = np.log(metadata['bc'])\n",
        "\n",
        "# the ufp_cat and bc_cat columns are numbers. we can either keep them numeric or categorical\n",
        "# first give them more meaningful names, we'll include abreviations of quintile continuous\n",
        "metadata = metadata.rename(columns={'ufp_cat': 'ufp_quint_cont', 'bc_cat': 'bc_quint_cont'}, errors = 'raise')\n",
        "\n",
        "# create categorical columns that are strings.\n",
        "# if we want to use a classification algorithm (i.e. softmax activation function), the outcome (i.e., pollutant we want to predict) will need to be strings\n",
        "metadata[['ufp_quint_cat', 'bc_quint_cat']] = metadata[['ufp_quint_cont', 'bc_quint_cont']].astype(str)\n",
        "metadata[['ufp', 'ufp_quint_cont', 'bc', 'bc_quint_cont']] = metadata[['ufp', 'ufp_quint_cont', 'bc', 'bc_quint_cont']].astype('float32')\n",
        "\n",
        "# set seed and split metadata into train, val, and test\n",
        "np.random.seed(1997)\n",
        "train, validate, test = np.split(np.random.choice(pd.unique(metadata['geohash']), size = len(pd.unique(metadata['geohash'])), replace = False), [int(0.7*len(pd.unique(metadata['geohash']))), int(0.85*len(pd.unique(metadata['geohash'])))])\n",
        "# np.random.choice puts the unique geohashs in a random order (replace = false means each unique geohash is sampled once) (there is no probability, so the default is each geohash has the same probability of being selected)\n",
        "# np.split cuts the vector a specified locations, in this case the locations are 0.7 the length and 0.85 the length, this gives 0.7-0.15-0.15\n",
        "\n",
        "metadata.loc[metadata['geohash'].isin(train), 'set'] = 'train'\n",
        "# metadata['geohash'].isin(train) give a vector of T/F, it evaluates if the geohash row value in train\n",
        "# metadata.loc[...] locates the rows based on the T/F vector\n",
        "# ..., 'set'] = 'train' creates a new column 'set' and fills it with 'train', but only in the geohash that are in the train\n",
        "\n",
        "metadata_nsites = metadata.shape[0] # we want to keep track of how many rows we have in the metadata. this will be useful down below\n",
        "\n",
        "# inspect the metadata\n",
        "metadata\n",
        "# slide the scrolling bar of the output to the right, we can see that only the training set has been assigned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEnuDlA6hAn7"
      },
      "outputs": [],
      "source": [
        "# repeat for validate and test sets\n",
        "metadata.loc[metadata['geohash'].isin(validate), 'set'] = 'validate'\n",
        "metadata.loc[metadata['geohash'].isin(test), 'set'] = 'test'\n",
        "metadata = metadata.sort_values(by='file')\n",
        "\n",
        "metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIzwT3jP7PBN"
      },
      "source": [
        "**Check the Data Split**  \n",
        "\n",
        "We can count how many rows are in each set. We expect it to be roughly 70%, 15%, and 15% in the train, validate, and test sets. It probably won't be exact because we randomly split by geohash code and each geohash code can have different numbers of road segments within it.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju9HUsQbhQVV"
      },
      "outputs": [],
      "source": [
        "# count number of rows in each set\n",
        "metadata.set.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9KL6tfPV_jN"
      },
      "source": [
        "# Image Filepaths\n",
        "\n",
        "The metadata has the image file names in it, but we should specify part of the filepaths as well. Each road segment has two satellite images (zoom 18 and 19) and two street view images (up and down the street).  \n",
        "\n",
        "Recall the organization of the folders:  \n",
        "> images/\n",
        ">> images_18/  \n",
        ">> images_19/  \n",
        ">> images_angle1/  \n",
        ">> images_angle2/  \n",
        "\n",
        "Look back at output of the metadata. For site id = 1, the filename is \"00001.jpg\". Each image folder has a file named \"00001.jpg\". If you want one of the street-view images for site id = 1, you'll find it here:\n",
        "> *images/images_angle1/00001.jpg*\n",
        "\n",
        "The other street-view images for site id = 1 is here:\n",
        "> *images/images_angle2/00001.jpg*\n",
        "\n",
        "If you want the zoom 18 satellite image for site id = 1, you'll find it here:\n",
        "> *images/images_18/00001.jpg*  \n",
        "\n",
        "The zoom 18 satellite image for site id = 5911 is here:\n",
        "> *images/images_18/05911.jpg*\n",
        "\n",
        "You can see how changing the filepath is a way to select a specific image for a given site. We can create new column that points to the zoom 18 satellite images by pasting \"images_18/\" to the values in the file column.\n",
        "\n",
        "If we are training on satellite images, we want to train on both zoom levels. To do so we'll create two copies of the metadata, assign one of the pictures to each copy in a new \"sat_file\" column, and then bind the rows of the two data sets.  \n",
        "\n",
        "The same goes for street-view images, we want to train on images from both directions. We'll do the same except with the \"images_angle1\" and \"images_angle2\" in a new \"str_file\" column.   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6CUsImihEXv"
      },
      "outputs": [],
      "source": [
        "# create two copies of the metadata called s1 and s2\n",
        "s1, s2 = metadata.copy(), metadata.copy()\n",
        "\n",
        "# # define location of satellite images, zoom level 18 for one copy and zoom level 19 for the other\n",
        "s1['sat_file'] = 'images_18/' + s1['file']  # create a column and fill it with the zoom 18 filepaths\n",
        "s2['sat_file'] = 'images_19/' + s2['file']  # create a column with the same name and fill it with the zoom 19 filepaths\n",
        "\n",
        "# # do the same for street images\n",
        "s1['str_file'] = 'images_angle1/' + s1['file'] # create a column and fill it with the angle1 filepaths\n",
        "s2['str_file'] = 'images_angle2/' + s2['file'] # create a column with the same name and fill it with the angle2 filepaths\n",
        "\n",
        "# bind s1 and s2 together. now the metadata is twice as long as it was before\n",
        "if metadata.shape[0] < metadata_nsites*2:  # if the metadata has fewer rows than 2 x the number of sites, then we bind s1 and s2. This makes sure that if we run this cell twice, we don't keep doubling the number of rows in the metadata\n",
        "  metadata = pd.concat([s1, s2])\n",
        "\n",
        "metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxmtDiTxaD1R"
      },
      "source": [
        "# Managing Images\n",
        "\n",
        "\n",
        "\n",
        "Let's take a look as some images. It's neat to see all four images associated with a site, but more importantly, this is a good way to confirm that the image paths are correct. I have spent way too many hours troubleshooting a model that won't train only to realize that it was bad filepaths to images. When troubleshooting code, check data types and check file paths - those are easy mistakes but easy fixes!  \n",
        "\n",
        "Recall the organization of the folders:  \n",
        "> images/\n",
        ">> images_18/  \n",
        ">> images_19/  \n",
        ">> images_angle1/  \n",
        ">> images_angle2/  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_HdcOdJhUBi"
      },
      "outputs": [],
      "source": [
        "# take a look at the images\n",
        "# this is will also confirm that the image_dir actually leads to the images, this is a good check that helps with troubleshooting\n",
        "\n",
        "# the parent directory for images, we will use this later on in the code. we will use this as a \"global variable\", which will be explained in a text cell further below\n",
        "image_dir = \"/content/images/\"\n",
        "\n",
        "np.random.seed(seed = None) # unset the random seed so we get a random image\n",
        "random_image_number = np.random.choice(metadata['file'])  # select a random image file name\n",
        "np.random.seed(1997) # set the random seed back for reproducibility\n",
        "\n",
        "# read in each type of image for the randomly selected site\n",
        "# notice how the parent directory gets pasted with the type of image directory and then the image file name?\n",
        "  # this is similar to what we did when creating the str_file and sat_file columns, we pasted file paths with the image file names\n",
        "img_18 = mpimg.imread(image_dir + 'images_18/' + random_image_number)\n",
        "img_19 = mpimg.imread(image_dir + 'images_19/' + random_image_number)\n",
        "imgangle_1 = mpimg.imread(image_dir + 'images_angle1/' + random_image_number)\n",
        "imgangle_2 = mpimg.imread(image_dir + 'images_angle2/' + random_image_number)\n",
        "\n",
        "# set up the 4 subplots in 2x2 and plot show the images\n",
        "f, axarr = plt.subplots(2, 2, figsize = (10,10))\n",
        "axarr[0,0].imshow(img_18)\n",
        "axarr[0,1].imshow(img_19)\n",
        "axarr[1,0].imshow(imgangle_1)\n",
        "axarr[1,1].imshow(imgangle_2)\n",
        "\n",
        "# you can run this code chunk several times in order to see different pictures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuWV8D8lkVVH"
      },
      "source": [
        "In the plots above, notice the x and y axis values. Those are pixels. We have images with different resolutions. Keep that in mind as we work through the code.  \n",
        "\n",
        "Here's another check we can do before we start training. It counts the number of files in a directory. We expect there to be the same number of files in each folder. We make a loop to count in each of the 4 sub directories with images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYRcJ6sikT2r"
      },
      "outputs": [],
      "source": [
        "# subfolders we want to count the number of files in\n",
        "sub_dirs = ['images_18/', 'images_19/', 'images_angle1/', 'images_angle2/']\n",
        "\n",
        "for i in sub_dirs: # for each sub-directory, start the count at zero and then use the second loop to count all the files\n",
        "  count = 0\n",
        "  for path in os.listdir(image_dir + i):  # recall that we defined image_dir above when checking images, you want to use this every time because that means if you need to make a change, you are only changing in one spot!\n",
        "      # check if current path is a file\n",
        "      if os.path.isfile(os.path.join(image_dir + i, path)):\n",
        "          count += 1  # += adds to and redefines the variable\n",
        "  print(i, ' file count:', count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZukdIs9DanHO"
      },
      "source": [
        "So we are missing some files in *images_19*. Do you think we can still train a model even though we are missing 5 images? Absolutely. Might we get tripped up somewhere along the way? Also yes! Keras will automatically skip any invalid filenames (i.e., missing images), so you will be able to train the models and generate predictions. However you may run into problems when you try to incorporate the predictions into the metadata because you won't know which sites have missing images.\n",
        "\n",
        "The code below identifies which files are in the *images_18* folder but not in the *images_19* folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXoCajy_yzH2"
      },
      "outputs": [],
      "source": [
        "# list the files that are in each directory\n",
        "list18 = os.listdir(image_dir + 'images_18/') # our global variable \"image_dir\" again!\n",
        "list19 = os.listdir(image_dir + 'images_19/')\n",
        "\n",
        "# which files in images_18 are not in images_19?\n",
        "missing_19 = list(set(list18).difference(list19))\n",
        "missing_19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-iigA2g2-IJ"
      },
      "source": [
        "Luckily, we have the missing files in *missing_images_19*. You can move them over to the correct folder.\n",
        "\n",
        "If you want, you can go ahead and train the models without these missing images (i.e. don't run the code below and cary on) to see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGHv3gn91Qxy"
      },
      "outputs": [],
      "source": [
        "for i in missing_19:\n",
        "  # renames the files, instead of changing the file's name, we change the directory\n",
        "  os.rename(image_dir + 'missing_images_19/' + i, image_dir + 'images_19/' + i)\n",
        "\n",
        "# # after moving the files, you can go back and check the counts in the folders\n",
        "# # you can also try this:\n",
        "# list(set(list19).difference(list18))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl-VlUK-yzoz"
      },
      "source": [
        "# Train Models\n",
        "\n",
        "Now that the metadata is ready and we are happy with our images we can start setting up out models.\n",
        "\n",
        "We start by definine our training and validation image data generators. These are functions that pre-process the data to prepare it for our CNN model. Here we are using *ImageDataGenerator().flow_from_dataframe()* which uses the metadata variable that we have set up. It expects the metadata to have a column indicating image filenames and a column indicating image labels (i.e., air pollution level). *ImageDataGenerator()* will resize images to a target size. Different model architectures have different acceptable model sizes. Xception's default is 299 x 299, but it is pretty flexible and can take anything larger than 150 x 150. EfficientNetB2 on the other hand requires 260 x 260. You can read more about these different architectures in the Keras documentation.     \n",
        "\n",
        "\n",
        "Instead of using a dataframe input, we could have used *flow_from_directory()* which uses directories (i.e., folders) for each category of image. That way, the directory acts as a label for all the images within it. For us, that would require a folder for each quintile. Here is a tutorial on using *flow_from_directory()*:    \n",
        "https://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxIthiLAzBEu"
      },
      "outputs": [],
      "source": [
        "# define the training and validation generators\n",
        "# now make one for each\n",
        "generator = K.preprocessing.image.ImageDataGenerator(preprocessing_function=K.applications.xception.preprocess_input,\n",
        "                                                      horizontal_flip=True,\n",
        "                                                      vertical_flip = True) # set to false if using street images\n",
        "\n",
        "\n",
        "train_generator = generator.flow_from_dataframe(dataframe=metadata.loc[metadata['set']=='train', ['ufp', 'sat_file']].reset_index(drop=True),\n",
        "                                                directory= image_dir,  # we already defined our image directory when we were ploting the pictures\n",
        "                                                x_col= 'sat_file',  # this is the filename of the images\n",
        "                                                y_col='ufp',  # this is the label for the images\n",
        "                                                class_mode= 'raw',    # is the outcome continuous ('raw') or categorical\n",
        "                                                target_size=(240, 240),  # all of our images will be resized to 240 x 240\n",
        "                                                color_mode='rgb',  # this is the type of image. Note that some models don't work with grayscale\n",
        "                                                batch_size=64,  # seems about the same speed with 32 or 64\n",
        "                                                shuffle=True)\n",
        "\n",
        "validate_generator = generator.flow_from_dataframe(dataframe=metadata.loc[metadata['set']=='validate', ['ufp', 'sat_file']].reset_index(drop=True),\n",
        "                                                     directory= image_dir,\n",
        "                                                     x_col= 'sat_file',\n",
        "                                                     y_col='ufp',\n",
        "                                                     class_mode='raw',  # is the outcome continuous ('raw') or categorical\n",
        "                                                     target_size=(240, 240),\n",
        "                                                     color_mode='rgb',\n",
        "                                                     batch_size=64,\n",
        "                                                     shuffle=False)\n",
        "\n",
        "# https://vijayabhaskar96.medium.com/tutorial-on-keras-flow-from-dataframe-1fd4493d237c\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ0IfTcCKen9"
      },
      "source": [
        "\n",
        "Notes on input size:  \n",
        "https://datascience.stackexchange.com/questions/89860/input-shape-of-an-xception-cnn-model  \n",
        "Several models are flexible on input size as long as it is over a certain minimum. If too small, the convolutions will transform the image data into nothing. The above post says Xception is probably best with 299x299 because that was the original set up, but you shouldn't resize images larger because you are not adding any new information and you're just adding more parameters (i.e. making it slower and larger) for no benefit. Other model architectures such as efficientNet requires a specific size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKhzVt6sdz20"
      },
      "source": [
        "\n",
        "\n",
        "**Change Control**\n",
        "\n",
        "The code above is fine and works, but how many changes in the code would we have to make if we wanted to train on street-view images instead of satellite images? In the code above you would have to make five changes and there would be more changes in following code as well (e.g., model name when you are saving it). Instead of changing the code you could copy and paste it and have two different versions in different cells or maybe have different notebooks. Either way, if you make changes to your code, you would have to implement it in many spots. You can see how things might get out of control if you're making changes and how errors can insidiously sneak into your code.   \n",
        "\n",
        "\n",
        "An easier way to handle this is to think of all the changes you might want to make (e.g., satellite vs street, UFP vs BC, etc.) and keep them in one place. This is similar to how we defined *image_dir* in one place and then kept using it in other code chunks.  A variable that impacts multiple functions and code chunks can be referred to as a \"global variable\". When figuring out if a variable will be \"global\" for you code, it is helpful to think of which changes might impact the code in multiple spots. For example, if we want to train on UFP categorical quintiles, then that impacts which column in the metadata is the *y_col*, but it also impacts the *class_mode* (*'categorical'* instead of *'raw'*). You can see how this can lead us to using some *if else* statements to set global variables. Without even thinking of how to code, this might be how you would explain the changes:\n",
        "\n",
        "\n",
        "> if I want to predict UFP categorical quintiles:\n",
        ">> -then I need the *y_col* to be *ufp_quint_cat*   \n",
        ">> -I need *class_mode* to be set to *'categorical'*  \n",
        ">> -I need to use a *'soft_max'* activation function  \n",
        ">> -I care about prediction accuracy  \n",
        ">> -I want to maximize prediction accuracy  \n",
        "\n",
        "> but if I want to predict UFP continuous:\n",
        ">> -then I need the *y_col* to be *ufp*   \n",
        ">> -I need *class_mode* to be set to *'raw'*  \n",
        ">> -I need a *'linear'* activation function  \n",
        ">> -I care about mean prediction errors  \n",
        ">> -I want to minimize prediction errors\n",
        "\n",
        "Thinking through the problem sets us up to code a solution!\n",
        "\n",
        "We find it can be helpful to combine the *input()* function prompts with some *if else* statements. That takes a little more planning and work up front, but it can make your life much easier in the long run and is a good approach for avoiding human errors.  \n",
        "\n",
        "Below is some code that you can play around with to get comfortable with using *input()* with *if else* statements. It won't affect any other code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz4dpCvITFCz"
      },
      "outputs": [],
      "source": [
        "########## running the code will not affect any other code ##########\n",
        "# this code shows you how the input function behaves\n",
        "# you can play around with it\n",
        "\n",
        "serious_answer = input(\"Are you having fun? \")  # this will open a prompt below, input your answer and press return/enter\n",
        "print('Your answer was: '+ serious_answer) # this will print text with your answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kAV3LamcxzR"
      },
      "outputs": [],
      "source": [
        "########## running the code will not affect any other code ##########\n",
        "# this code shows you how you can use the input with if else statements to set up your model\n",
        "# you can play around with it\n",
        "\n",
        "type_of_food = input(\"Do you prefer pizza or pasta? \")  # this will open a prompt below, input your answer and press return/enter\n",
        "\n",
        "if type_of_food == 'pizza': # if you wrote pizza, then it will assign the value 'beer' to the beverage variable\n",
        "  beverage = 'beer'\n",
        "else:\n",
        "  if 'past' in type_of_food: # if 'past' is in what you wrote, then it will assign the value 'wine' to the beverage variable. notice how this evaluation is more tolerant of typos\n",
        "    beverage = 'wine'\n",
        "  else:\n",
        "      print('!!!Unexpected value in food type') # if you wrote something other than pizza or pasta, then it will print this message\n",
        "\n",
        "print('Lets go have some ' + type_of_food + ' and '+ beverage + '!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13_V8iO2BkQD"
      },
      "source": [
        "# Global Variables - One Change in One Place\n",
        "\n",
        "Now that you have an idea of what using *input()* with *if else* statements can do, let's create our set our global variables. Think of this as \"One Change in One Place\". This cell will set the values for a many of the variables we'll use when trainig the model, saving outputs, and generating predictions. It can be useful to have \"my_\" at the start of all the variables that are defined in this chunk. That way if you're reading code somewhere else and see a variable that starts with \"my_\", you know exactly where it was initially defined. You can use something other than \"my_\", whatever you find useful. I have seen other people write their global variables in all caps.     \n",
        "\n",
        "As you go through this exercise, you can think of other arguments you might have wanted to include in this section (e.g., *taget_size* or the *join_count* cutoff we used to prepare the metadata above). You may also find that you want fewer inputs here - lots of inputs can make it hard to follow the code. Maybe you are only interested in using the Xception architechture. There is a balance between elegance and readability. When developing code, we usually:\n",
        "    \n",
        "\n",
        "1.   Write code for one use case (e.g., continuous UFP trained on satellite images using Xception architecture)\n",
        "2.   Think about the other use cases\n",
        "3.   Write out the if else for all the changes that would need to be made\n",
        "4.   Create One in Change One Place code cell and replace values with what are now your global variables\n",
        "\n",
        "\n",
        "\n",
        "Another advantage of this \"One Change in One Place\" appoach is that you can easily turn it into loop(s) to train multiple models. What if you want to develop UFP and BC models trained on satellite and street-view images (i.e., four models)? You could set up loops to train them all one after the other.\n",
        "\n",
        "The choices you'll make in the code cell below:\n",
        "1. Type of images, satellite or street.\n",
        "2. Pollutant you will model, UFP or BC.\n",
        "3. Numeric form of pollutant, continuous or quintiles. If quintile, it can be numeric or categorical.\n",
        "4. How long to train, put 10 epochs in the interest of time.\n",
        "5. What model architecture you'll use, Xception, ResNet50V2 or DensNet121.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b1mcZk4elQn"
      },
      "outputs": [],
      "source": [
        "########################### ONE CHANGE ONE PLACE #################################\n",
        "# this code will affect the rest of the code. it asks for inputs from you and sets the names of the parameters for the model\n",
        "# I should add more comments here explaining each one\n",
        "\n",
        "# this opens the prompt below asking for user input. once you hit return, the value will be assinged to my_type_of_images_long\n",
        "my_type_of_images_long = input(\"Train on which kind of images: satellite or street? \")\n",
        "\n",
        "# this detects 'sat' in the input, that way we can just write 'sat' or 'satellite' or even misspell anything other than the first three letters\n",
        "if 'sat' in my_type_of_images_long:  # if the input contains 'sat' then:\n",
        "  my_type_of_images = 'sat'  # we will use 'my_type_of_images' when naming our output files, this gives a consistent name to our outputs regardless of how we spelled the input\n",
        "  metadata['train_file'] = metadata['sat_file']  # create a column that identifies what images we will train the models on, this shows names and type\n",
        "  my_vert_flip = True   # during training, we want vertical flipping for satellite images, but not for street images\n",
        "else:\n",
        "  if 'str' in my_type_of_images_long: # if the input contains 'str' then:\n",
        "    my_type_of_images = 'str'  # we will include 'str' in the names output files\n",
        "    metadata['train_file'] = metadata['str_file']  # create a column for training, but in this case it is filled with the values of the street view file names\n",
        "    my_vert_flip = False  # during training, we do not want vertical flipping for street images\n",
        "  else:\n",
        "      print('!!!TYPO in train image type')  # if the input does not contain 'sat' or 'str', then it will print this message\n",
        "\n",
        "my_ufp_or_bc = input(\"What pollutant: ufp or bc? \") # this asks for user input and sets the pollutant to either UFP of BC\n",
        "my_pred_col = my_ufp_or_bc + '_pred'  # when we generate predictions, we will create a new colum named either ufp_pred or bc_pred\n",
        "\n",
        "# do we want to train on the continous values or the quintiles?\n",
        "my_cont_or_quint_long = input(\"What form of the pollutant: continuous or quintiles? \")\n",
        "\n",
        "# like with type of images, this will detect the first couple of letters, which lets us have some typos\n",
        "if 'cont' in my_cont_or_quint_long:\n",
        "  my_cont_or_quint = 'continuous' # like with type of images, this sets the variable that we will use in other spots\n",
        "  my_y_col = my_ufp_or_bc  # this set the y column for training and predictions\n",
        "  my_target_class_mode = 'raw'  # this tells the model that the y column is continous (i.e. treat as it is, 'raw')\n",
        "  my_activation = 'linear'  # the activation function is linear for continuous y, bascially a linear regression\n",
        "  my_output_units = 1  # we want a single value as the output, that is the form of output from linear activation\n",
        "  my_loss = rmse  # the loss function in the model, root mean squared error for continous outcomes. we difeined this at the start. we could also use 'mae' which is mean absolute error\n",
        "  my_loss_metric = rmse  # the metric to track during training. this will be tracked in the training set and in the validation set\n",
        "  my_metric_of_interest = 'val_rmse'  # the callbacks will monitor root mean squared error in the validation set, more on this below\n",
        "  my_max_or_min_metric = 'min'  # the goal is to minimize rmse\n",
        "  my_type_of_y = 'numeric' # this is just to help with the printed output at the end of this chunk\n",
        "\n",
        "else:\n",
        "  if 'quint' in my_cont_or_quint_long:  # instead of training on continous values, we could train on the quintiles\n",
        "    my_cont_or_quint = 'quintiles'   # this sets the variable that we can use in other places\n",
        "    my_type_of_quint_y_long = input(\"What form of quintile: numeric or categorical? \") # even when using quintiles, we could treat them as continuous numbers from 1 - 5 or as seperate categories\n",
        "    if 'cat' in my_type_of_quint_y_long: # if we want to treat the quintiles as catigorical\n",
        "      my_type_of_y = 'categorical'\n",
        "      my_y_col = my_ufp_or_bc + '_quint_cat' # this set the y column for training and predictions to either ufp_quint_cat or bc_quint_cat, recall that we created this column as strings of 1 - 5\n",
        "      my_target_class_mode = 'categorical' # this tells the model that the y column is categorical\n",
        "      my_activation = 'softmax'  # categorical uses a softmax activation function\n",
        "      my_output_units = 5  # now that we are looking at 5 categories, we want an output that is the probability of each category\n",
        "      my_loss = 'categorical_crossentropy' # the loss function in the model, categorical_crossentropy for categorical outcomes.\n",
        "      my_loss_metric = 'accuracy' # the metric to track during training. this will be tracked in the training set and in the validation set. for categorical, we are interested in accuracy\n",
        "      my_metric_of_interest = 'val_accuracy'  # the callbacks will monitor accuracy in the validation set, more on this below\n",
        "      my_max_or_min_metric = 'max' # we want maximum accuracy\n",
        "    else:\n",
        "      if 'num' in my_type_of_quint_y_long:\n",
        "        my_type_of_y = 'numeric'  # we could chose to train on quitiles, but treat them as continous numbers from 1 - 5\n",
        "        my_y_col = my_ufp_or_bc + '_quint_cont'  # this set the y column for training and predictions, recall that this was the original quintile column of numbers from 1 - 5\n",
        "        my_target_class_mode = 'raw'  # because we are treating them as continuous, the rest of the parameters are for linear\n",
        "        my_activation = 'linear' # activation , ouptus, and loss are all the same as for continuous\n",
        "        my_output_units = 1\n",
        "        my_loss = rmse\n",
        "        my_loss_metric = rmse\n",
        "        my_metric_of_interest = 'val_rmse'\n",
        "        my_max_or_min_metric = 'min'\n",
        "      else:\n",
        "        print('!!!TYPO in Form of quintile')\n",
        "  else:\n",
        "    print('!!!TYPO in continuous vs quintile')\n",
        "\n",
        "# how long do we want to train the model for? typically we will train up to 100 epochs, but in the interest of time, set to 10\n",
        "my_number_of_epochs = input(\"Train for how many epochs? 10 epochs takes about 25 mins \")\n",
        "\n",
        "# which model architecture do you want to use?\n",
        "my_model_architecture_long = input(\"What model architecture do you want to use? xception, resnet50V2, or densenet121? \")\n",
        "\n",
        "# this allows us to make some typos in model architecture input. If you get the first three letters correct, then it'll set model architecture name in a predictable way\n",
        "if 'xce' in my_model_architecture_long:\n",
        "  my_model_architecture = 'xcep'\n",
        "  my_pre_pro_fn = K.applications.xception.preprocess_input # the different architectures have different preprocessing functions\n",
        "  my_batch_size = 32  # batch size is the number of observations the model will use for gradient descent\n",
        "else:\n",
        "  if 'resn' in my_model_architecture_long:\n",
        "    my_model_architecture = 'resn'\n",
        "    my_pre_pro_fn = K.applications.resnet_v2.preprocess_input\n",
        "    my_batch_size = 32\n",
        "  else:\n",
        "    if 'dens' in my_model_architecture_long:\n",
        "      my_model_architecture = 'dens'\n",
        "      my_pre_pro_fn = K.applications.densenet.preprocess_input\n",
        "      my_batch_size = 32  # This model is a bit more comuationally intensive and required a smaller batch size\n",
        "    else:\n",
        "      print('!!!TYPO in model architecture ')\n",
        "\n",
        "# this gives a printout that is easy to read. it is handy for:\n",
        "  # 1. checking to make sure the inputs are what you want\n",
        "  # 2. when you come back after hours of training and ask yourself: \"what did I just train?\"\n",
        "print('You are using ' + my_model_architecture_long + ' trained on ' + my_type_of_images + ' images to predict the ' + my_y_col + ' column (' + my_ufp_or_bc +' '+ my_cont_or_quint +' as '+ my_type_of_y + ')')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_QawqLD3Zhb"
      },
      "source": [
        "With your global variables set, you can now set up image pre-processing exactly how you want it without having to modify the code in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPRrSlcgL6bl"
      },
      "outputs": [],
      "source": [
        "generator = K.preprocessing.image.ImageDataGenerator(preprocessing_function= my_pre_pro_fn, # this is the preprocessing function defined above\n",
        "                                                      horizontal_flip=True, # we flip the images for more robust training\n",
        "                                                      vertical_flip = my_vert_flip) # set to false if using street images\n",
        "\n",
        "# specify the dataframe for training. we just need the training set with the target column (i.e., the 'y', the pollutant we want to train on) and the filepaths of the training imagaes\n",
        "train_generator = generator.flow_from_dataframe(dataframe=metadata.loc[metadata['set']=='train', [my_y_col, 'train_file']].reset_index(drop=True),\n",
        "                                                directory= image_dir,  # we already defined our image directory when we were ploting the pictures\n",
        "                                                x_col= 'train_file',  # recall that this was set to either str_file or sat_file\n",
        "                                                y_col= my_y_col,  # this is either ufp, ufp_cat (1 - 5 continuous), ufp_cat_str (1 - 5 categorical), or the bc equivalent\n",
        "                                                class_mode= my_target_class_mode,    # is the outcome continuous ('raw') or categorical?\n",
        "                                                target_size=(240, 240),\n",
        "                                                color_mode='rgb',\n",
        "                                                batch_size= my_batch_size,\n",
        "                                                shuffle=True)\n",
        "\n",
        "# the validate is the same except with the validation set\n",
        "validate_generator = generator.flow_from_dataframe(dataframe=metadata.loc[metadata['set']=='validate', [my_y_col, 'train_file']].reset_index(drop=True),\n",
        "                                                     directory= image_dir,\n",
        "                                                     x_col= 'train_file',\n",
        "                                                     y_col= my_y_col,\n",
        "                                                     class_mode= my_target_class_mode,\n",
        "                                                     target_size=(240, 240),\n",
        "                                                     color_mode='rgb',\n",
        "                                                     batch_size= my_batch_size,\n",
        "                                                     shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1NSypttmkW-"
      },
      "source": [
        "You will see output telling you how many validated image filenames were found. Recall that we had 3362 sites, with two images each that makes 6724 images. That is reassuring! If any images were missing, it will tell you here.   \n",
        "\n",
        "***Try later:*** *You can start a new runtime and try training on satellite images without moving the \"missing_images_19\" images. How does that impact the output when you run the code cell above?*\n",
        "\n",
        "\n",
        "# Define Callbacks\n",
        "\n",
        "\"Callbacks\" are actions that we want Keras to do during trainig and we can set conditions for those actions based on model performance in the validation or training sets. In our case we want to define criteria for early stopping and reducing learning rates. We also want to save a log of model performance and save only the best model. We could save the model from each epoch, but that can take up a lot of space and we probably aren't interested in anything other than the best model. If you're tight on space, you can always just save the models wieghts instead of the entire model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f-IOnuIzI9m"
      },
      "outputs": [],
      "source": [
        "# define callbacks\n",
        "\n",
        "# stop the training if the metric of interest (RMSE or accuracy) doesn't improve during a certain number of epochs (set to 15 here). there's no sense in continuous training if the model isn't leraning\n",
        "early_stopping = K.callbacks.EarlyStopping(monitor= my_metric_of_interest, patience=15, mode= my_max_or_min_metric)\n",
        "\n",
        "# reduce learning by a certain factor if the metric of interest (RMSE or accuracy) doesn't improve during a certain number of epochs (set to 5 here)\n",
        "reduce_lr_on_plateau = K.callbacks.ReduceLROnPlateau(monitor= my_metric_of_interest, factor=0.1, patience=5, mode= my_max_or_min_metric, verbose=1) # verbose = 1 gives us output every time the learning rate changes\n",
        "\n",
        "# save a log of model performance. notice how the file name will change depending on which model we are training (pollutant_type-of-image_architecture_max-epochs)\n",
        "csv_logger = K.callbacks.CSVLogger('/content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model_log/' + my_y_col + '_' + my_type_of_images + '_' + my_model_architecture +'_model_logger_'+ my_number_of_epochs+'e_lr10e-3.csv')\n",
        "\n",
        "# save the best model\n",
        "model_checkpoint = K.callbacks.ModelCheckpoint('/content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model/' + my_y_col + '_' + my_type_of_images + '_' + my_model_architecture + '_best_model_'+ my_number_of_epochs+'e_lr10e-3.hdf5', monitor= my_metric_of_interest, mode= my_max_or_min_metric, save_weights_only=False, save_best_only=True)\n",
        "\n",
        "# this will print out so you can see what the model name will be\n",
        "'/content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model/' + my_y_col + '_' + my_type_of_images + '_' + my_model_architecture + '_best_model_'+ my_number_of_epochs+'e_lr10e-3.hdf5'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozxSwv4em4BI"
      },
      "source": [
        "# Compile and Train Model\n",
        "\n",
        "Next we define the model compile function. Compiling sets the shape of input tensors, the model architecture, any initial weights, the activation function, and several other model parameters you can see in the code below.\n",
        "\n",
        "If we are training on a continous y, then we use a linear activation function and RMSE as the loss function. If we are training on a categorical y, then we use a softmax activation function with accuracy as the loss function. Now you can see how setting all those global variables in one place is paying off!\n",
        "\n",
        "**Initial Weights**\n",
        "\n",
        "We'll use inital weights from a model that has already been trained. This is called \"transfer learning\". Here we'll use weights from models trained on the \"ImageNet\" database. It has millions of labeled images and is used in research. The images have nothing to do with air pollution, but it costs us nothing to use these initial weights so we may as well. If we had some other trained models, we could try using their initial weights as well.  \n",
        "\n",
        "***Try later:*** *You could try training without using imagenet initial weights or with the initial weigths from some other models. How does that impact model performance? Do you need to train longer in order to get good perfromance?*\n",
        "\n",
        "Note on \"include_top\":\n",
        "https://stackoverflow.com/questions/46036522/defining-model-in-keras-include-top-true\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FebFURT1mzk1"
      },
      "outputs": [],
      "source": [
        "def get_compiled_model():\n",
        "    # define model input. Xception and ResNet50 can take various input shapes, but EfficientNetB2 requires 260 x 260, 3 (the 3 is the color channels)\n",
        "    model_input = K.layers.Input(shape=(240, 240, 3), dtype='float32', name='input')\n",
        "\n",
        "    # define base, one of three models. There are many others to pick from. We are using imagenet initial weights for all of them and doing so requires include_top = False\n",
        "    if my_model_architecture == 'xcep':\n",
        "      conv_base = K.applications.Xception(include_top=False, weights= \"imagenet\", input_tensor=model_input)  # include_top = False in order to use initial weights (i.e. transfer learning)\n",
        "    else:\n",
        "      if my_model_architecture == 'resn':\n",
        "        conv_base = K.applications.ResNet50V2(include_top=False, weights= \"imagenet\", input_tensor=model_input)\n",
        "      else:\n",
        "        if my_model_architecture == 'dens':\n",
        "          conv_base = K.applications.DenseNet121(include_top=False, weights= \"imagenet\", input_tensor=model_input)\n",
        "\n",
        "    model_output = K.layers.GlobalAveragePooling2D()(conv_base.output)\n",
        "    model_output = K.layers.Dense(units= my_output_units, activation= my_activation)(model_output)  # this is what makes it a regression model, for categorical: K.layers.Dense(units=10, activation='softmax')(model_output), class_mode in generators would also need to be changed to 'categorical'\n",
        "    model = K.models.Model(inputs=model_input, outputs=model_output)\n",
        "    model.compile(\n",
        "        optimizer=K.optimizers.Nadam(learning_rate = 0.01),  # may want to tune this learning rate. seems like 0.01 and 0.001 are alright. 0.0001 is too slow\n",
        "        loss = my_loss,   # we defined rmse in the first code chunk\n",
        "        metrics = my_loss_metric  # you can track multiple metrics if you want, e.g., for continuous you could do [rmse, 'mae']\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsA391nKzYZs"
      },
      "outputs": [],
      "source": [
        "# # we've included code showing what it would look like if you filled in all the values yourself instead of using variables defined\n",
        "# # this example is for an Xception model architechture with output of categorcial (i.e. classifier) quintiles\n",
        "#\n",
        "#   # define compiling model\n",
        "#   def get_compiled_model():\n",
        "#       # define model\n",
        "#       model_input = K.layers.Input(shape=(240, 240, 3), dtype='float32', name='input')\n",
        "#       conv_base = K.applications.Xception(include_top=False, weights= \"imagenet\", input_tensor=model_input)\n",
        "#       model_output = K.layers.GlobalAveragePooling2D()(conv_base.output)\n",
        "#       model_output = K.layers.Dense(units= 5, activation= 'softmax')(model_output)  # this is what makes it a regression model, for categorical: K.layers.Dense(units=10, activation='softmax')(model_output), class_mode in generators would also need to be changed to 'categorical'\n",
        "#       model = K.models.Model(inputs=model_input, outputs=model_output)\n",
        "#       model.compile(\n",
        "#           optimizer=K.optimizers.Nadam(learning_rate = 0.01),  # may want to tune this learning rate. seems like 0.01 and 0.001 are alright. 0.0001 is too slow\n",
        "#           loss = 'categorical_crossentropy',\n",
        "#           metrics = ['accuracy']\n",
        "#       )\n",
        "#       return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mroVhMX6gxJG"
      },
      "source": [
        "You have defined the get_compiled_model() function, now we want to actually compile it. We could have just run each line separetely, but we find it nice to have them all in once spot.\n",
        "\n",
        "> ***Compiling the model will take 2-10 seconds. You should see a progess bar.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCw9VAVhzogx"
      },
      "outputs": [],
      "source": [
        "model = get_compiled_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLcnXMRh6nfM"
      },
      "source": [
        "Now you are ready to train (i.e., fit) your model! If you set epochs to 10, this should take 20-30 mins. You will see progress bars for each epoch.\n",
        "\n",
        "> ***Training will take a while (~25 mins for 10 epochs) , so plan accordingly (i.e. go grab a coffee if you want!).***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3ldC7yKzK2K"
      },
      "outputs": [],
      "source": [
        "model.fit(train_generator,\n",
        "          validation_data=validate_generator,\n",
        "          epochs= int(my_number_of_epochs), # we have set it to only 10 epochs in the interest of time, this should take about 25  mins\n",
        "          steps_per_epoch=int(np.ceil(train_generator.samples/train_generator.batch_size)),\n",
        "          validation_steps=int(np.ceil(validate_generator.samples/validate_generator.batch_size)),\n",
        "          callbacks=[early_stopping, reduce_lr_on_plateau, csv_logger, model_checkpoint]) # all the callbacks we defined earlier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ_q8NGUsfAN"
      },
      "source": [
        "You have just trained your model using the training and validation sets. The log file that was saved will tell you how the model performed in the training and validation sets, but those values will be biased. You should evaluate the model using data that the model has not seen, you should use the test set. Start by generating predictions in the test set.             \n",
        "\n",
        "# Generate Predictions\n",
        "\n",
        "As we did for the train and validation data sets, we will create the test generator. There is no *y_col* required for the test generator because it will simply be using the *x_col* (i.e., images) to generate predictions. Notice how the dataframe is using all the data, not just the test set data. We find it handy to generate predictions for all the data and then filter to just the test set when evaluating the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpHTkoRzBZiG"
      },
      "outputs": [],
      "source": [
        "\n",
        "generator = K.preprocessing.image.ImageDataGenerator(preprocessing_function= my_pre_pro_fn,\n",
        "                                                      horizontal_flip=False,\n",
        "                                                      vertical_flip = False) # you do not want to flip the images when generating predictions\n",
        "\n",
        "# you could just generate predictions for the test set, but here we will generate predictions for the entire data set\n",
        "# keep this in mind for when we later evaluate the results. at that point we will have to make sure we are only evaluating in the test set.\n",
        "test_generator = generator.flow_from_dataframe(dataframe=metadata[[my_y_col, 'train_file']].reset_index(drop=True),\n",
        "                                               directory= image_dir,\n",
        "                                               x_col= 'train_file',\n",
        "                                               class_mode= 'input', # or None?\n",
        "                                               target_size=(240, 240),\n",
        "                                               color_mode='rgb',\n",
        "                                               batch_size= my_batch_size,\n",
        "                                               shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y-BEfQCuKPW"
      },
      "source": [
        "Recall that we specified in the callbacks to save only the best model, so now we will load that model in order to generate predictions. We need to put the predictions somewhere, so we will also prepare a dataframe based on the metadata. The predictions will be added as a column to that dataframe. Useful information to have in this \"results\" dataframe are:  \n",
        "\n",
        "- *long* and *lat* (in order to plot)\n",
        "- *train_file* (if we have strange predictions, we may want to inspect the image)   \n",
        "- observed ufp, bc, and quintile (for comparing to predictions)\n",
        "- temp, relhum, winspeed (for temporal adjustments)\n",
        "- prediction (this is it what it is all about!)\n",
        "\n",
        "When generating predictions, Keras will skip any rows that don't have valid images. This means that if there are missing images you will have fewer predictions than there are rows in the results datafram. When setting up the results dataframe, it is good to verify the existence of all the image files references in the dataframe and to remove any rows that refer to invalid/missing images.  \n",
        "\n",
        "Finally, we'll save the dataframe as a csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnQnh3VjCRoB"
      },
      "outputs": [],
      "source": [
        "# load the model you just trained, this will take about 30s\n",
        "model = K.models.load_model('/content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model/' + my_y_col + '_' + my_type_of_images + '_' + my_model_architecture + '_best_model_'+ my_number_of_epochs+'e_lr10e-3.hdf5', custom_objects={'rmse': rmse})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be5nHdGy63Gm"
      },
      "source": [
        "> ***Generating predictions in the next code cell can take 1-2 minutes to complete, you'll get a progress bar***\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf2omqgaB-nL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# prepare a new dataframe based on the columns of interest in the metadata\n",
        "results = metadata[['id', 'long', 'lat', 'train_file', 'set', 'ufp', 'bc', 'ufp_quint_cont', 'bc_quint_cont', 'temp_f', 'relhum_pct', 'windspeed_kts']]\n",
        "\n",
        "# check to see if each image file exists and remove rows from the results dataframe for which image files don't exist.\n",
        "results['file_exists'] = results.apply(lambda row: os.path.isfile(image_dir + row.train_file), axis = 1)\n",
        "results = results.loc[results['file_exists'] == True]\n",
        "\n",
        "# if categorical, then the prediction gives a prediction for probability of each category. take the max\n",
        "# if continuous, then we just take the prediction.\n",
        "if 'cat' in my_y_col:\n",
        "  results[my_pred_col] = model.predict(x=test_generator, steps=int(np.ceil(test_generator.samples/test_generator.batch_size))).argmax(axis = -1)\n",
        "else:\n",
        "  results[my_pred_col] = model.predict(x=test_generator, steps=int(np.ceil(test_generator.samples/test_generator.batch_size)))\n",
        "\n",
        "# save the results as a csv file\n",
        "results.to_csv(path_or_buf='/content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model_predictions/' + my_y_col + '_' + my_type_of_images + '_' + my_model_architecture + '_model_predictions_'+ my_number_of_epochs+'e_lr10e-3.csv', index=False)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unUO37zfosDD"
      },
      "source": [
        "Congrats! You have trained a deep CNN and used it to generate air pollution predictions using images. What now?\n",
        "\n",
        "Now you should evaluate your model.\n",
        "\n",
        "Also, if you trained categorical quintiles, take a look at the observed vs the predicted quintiles. What do you notice? What are the ranges? Python uses \"zero-based indexing\", which means counting starts at 0, whereas in R counting starts at 1. Some people have strong opinions about this, but there's no \"right\" way, it's just important to keep in mind when swithcing between R and Python.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzncaa7VwmnK"
      },
      "source": [
        "\n",
        "**Pre-Trained Models**\n",
        "\n",
        "In this workshop we only had enough time for you to train for 10 epoch. Typically we would try training much longer (e.g., up to 100). We have provided models that we trained for 30 epochs that you can also use to generate a predictions file. You can run the code in the cells below to move the pre-trained models into your Google Drive and to get an overview of what the models they are. The code will also move the pre-trained model logs.  \n",
        "\n",
        "> ***Copying the files will take about 60 seconds***\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwWqYQs5_m8A"
      },
      "outputs": [],
      "source": [
        "# this cell will move the pre-trained models into your Google Drive folder\n",
        "\n",
        "# list the pre-trained models\n",
        "pre_trained_models = os.listdir('/content/drive/MyDrive/sharp_ml_course/pre_trained_models/ufp_colombia_pre_trained_model')\n",
        "\n",
        "# set the origin and destination directories\n",
        "from_dir = '/content/drive/MyDrive/sharp_ml_course/pre_trained_models/ufp_colombia_pre_trained_model/'\n",
        "to_dir = '/content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model/'\n",
        "\n",
        "# copy to the destination directory\n",
        "for i in pre_trained_models:\n",
        "  # renames the files, instead of changing the file's name, we change the directory\n",
        "  shutil.copyfile(from_dir + i, to_dir + i)\n",
        "\n",
        "# repeat for the model logs\n",
        "pre_trained_model_logs = os.listdir('/content/drive/MyDrive/sharp_ml_course/pre_trained_models/ufp_colombia_pre_trained_model_logs')\n",
        "from_dir = '/content/drive/MyDrive/sharp_ml_course/pre_trained_models/ufp_colombia_pre_trained_model_logs/'\n",
        "to_dir = '/content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model_log/'\n",
        "for i in pre_trained_model_logs:\n",
        "  shutil.copyfile(from_dir + i, to_dir + i)\n",
        "\n",
        "# print out names of the pre-trained models\n",
        "pre_trained_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrPUKQcXH6A9"
      },
      "outputs": [],
      "source": [
        "# this code extracts information from the model name in order to help us understand the models\n",
        "# this shows you how long names can seem clunky, but can be very useful. everything we need to know about the model is in the model's name\n",
        "# this code cell will also display some of the important model info in a table\n",
        "import re\n",
        "d = []\n",
        "for x in pre_trained_models:\n",
        "  d.append((x, re.findall('ufp_quint_cat|ufp_quint_cont|ufp', x)[0], re.findall('sat|str', x)[0], re.findall('xcep|resn|dens', x)[0], re.findall('model_(.*)e_lr', x)[0]))\n",
        "pre_trained_model_df = pd.DataFrame(d, columns = ('model_name', 'outcome', 'type_of_image', 'model_arch', 'number_epochs'))\n",
        "display(pre_trained_model_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrmD_-t-xnbl"
      },
      "source": [
        "The cell below will load one of the models in your *sharp_d2_UFP/output/model/* directory. That directory now contains the model you trained as well as the pre-trained models that we have provided. All those models are displayed in the table above. You can identify your model because *number_epochs* = 10, whereas *number_epochs* = 30 for all the pre-trained models.  \n",
        "\n",
        "**You don't need to run the code cell below** if you want to proceed with model evaluation using your model. You can run the code cell below ONLY if you want to load one of the pre-trained models to generate predictions for model evalutation (i.e. instead of using the model you trained).  \n",
        "\n",
        "> ***Loading model and generating predictions in the next code cell can take several minutes to complete. You will be prompted with an input below the code cell. Make sure to enter the row index to indicate which model you want to use when prompted.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaobsDySCAE1"
      },
      "outputs": [],
      "source": [
        "#####################################################################################################\n",
        "########### code for loading a pre-trained model and using it to generate predictions ###############\n",
        "#####################################################################################################\n",
        "\n",
        "#####################################################################################################\n",
        "######### DO NOT RUN THIS CODE IF YOU WANT TO USE THE MODEL YOU JUST TRAINED ########################\n",
        "#####################################################################################################\n",
        "selected_model = int(input('Using row index (i.e., number), indicate which model you want to use: '))\n",
        "print('You selected the ' + pre_trained_model_df.loc[selected_model, 'model_name'] + ' pre-trained model')\n",
        "\n",
        "# this loads the model\n",
        "model = K.models.load_model('/content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model/' + pre_trained_model_df.loc[selected_model, 'model_name'], custom_objects={'rmse': rmse})\n",
        "\n",
        "# this sets the values\n",
        "my_y_col, my_pred_col, my_type_of_images, my_model_architecture, my_number_of_epochs = pre_trained_model_df.loc[selected_model, 'outcome'], 'ufp_pred', pre_trained_model_df.loc[selected_model, 'type_of_image'], pre_trained_model_df.loc[selected_model, 'model_arch'], pre_trained_model_df.loc[selected_model, 'number_epochs']\n",
        "\n",
        "if my_model_architecture == 'xcep':\n",
        "  my_pre_pro_fn = K.applications.xception.preprocess_input\n",
        "else:\n",
        "  if my_model_architecture == 'resn':\n",
        "    my_pre_pro_fn = K.applications.resnet_v2.preprocess_input\n",
        "  else:\n",
        "    if my_model_architecture == 'dens':\n",
        "      my_pre_pro_fn = K.applications.densenet.preprocess_input\n",
        "\n",
        "\n",
        "# if the runtime has been disconnected/terminated the metadata will need to be read in again. as a shortcut, we have the processed metadata here\n",
        "metadata = pd.read_csv(\"/content/drive/MyDrive/sharp_ml_course/pre_trained_models/ufp_colombia_metadata.csv\")\n",
        "if my_type_of_images == 'sat':\n",
        "  metadata['train_file'] = metadata['sat_file']\n",
        "else:\n",
        "  metadata['train_file'] = metadata['str_file']\n",
        "\n",
        "my_batch_size = 32\n",
        "image_dir = '/content/images/'\n",
        "\n",
        "# need to redefine the test generator\n",
        "generator = K.preprocessing.image.ImageDataGenerator(preprocessing_function= my_pre_pro_fn,\n",
        "                                                      horizontal_flip=False,\n",
        "                                                      vertical_flip = False)\n",
        "test_generator = generator.flow_from_dataframe(dataframe=metadata[[my_y_col, 'train_file']].reset_index(drop=True),\n",
        "                                               directory= image_dir,\n",
        "                                               x_col= 'train_file',\n",
        "                                               class_mode= 'input',\n",
        "                                               target_size=(240, 240),\n",
        "                                               color_mode='rgb',\n",
        "                                               batch_size= my_batch_size,\n",
        "                                               shuffle=False)\n",
        "\n",
        "# prepare a new dataframe based on the columns of interest in the metadata\n",
        "results = metadata[['id', 'long', 'lat', 'train_file', 'set', 'ufp', 'bc', 'ufp_quint_cont', 'bc_quint_cont', 'temp_f', 'relhum_pct', 'windspeed_kts']]\n",
        "\n",
        "# check to see if each image file exists and remove rows from the results dataframe for which image files don't exist.\n",
        "results['file_exists'] = results.apply(lambda row: os.path.isfile(image_dir + row.train_file), axis = 1)\n",
        "results = results.loc[results['file_exists'] == True]\n",
        "\n",
        "# if categorical, then the prediction gives a prediction for probability of each category. take the max\n",
        "# if continuous, then we just take the prediction.\n",
        "if 'cat' in my_y_col:\n",
        "  results[my_pred_col] = model.predict(x=test_generator, steps=int(np.ceil(test_generator.samples/test_generator.batch_size))).argmax(axis = -1)\n",
        "else:\n",
        "  results[my_pred_col] = model.predict(x=test_generator, steps=int(np.ceil(test_generator.samples/test_generator.batch_size)))\n",
        "\n",
        "# save the results as a csv file\n",
        "results.to_csv(path_or_buf='/content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model_predictions/' + my_y_col + '_' + my_type_of_images + '_' + my_model_architecture + '_model_predictions_'+ my_number_of_epochs+'e_lr10e-3.csv', index=False)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o1OiusurceX"
      },
      "source": [
        "# Model Evaluation\n",
        "\n",
        "You may be coming back to a session that was disconnected/terminated and may need to re-define the global variables. You can do that up at the \"One Change One Place\" section or you can do it right here.  \n",
        "\n",
        "***You don't need to run the two code cells below if your session was not disconnected/terminated***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBcL-AvpUVmk"
      },
      "outputs": [],
      "source": [
        "# this code extracts information from the model names in order to help undestand what the model was trained on\n",
        "# get a list the available models, this will be a mix of pre-trained models and models you have trained today\n",
        "available_models = os.listdir('/content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model/')\n",
        "d = []\n",
        "for x in available_models:\n",
        "  d.append((x, re.findall('ufp_quint_cat|ufp_quint_cont|ufp', x)[0], re.findall('sat|str', x)[0], re.findall('xcep|resn|dens', x)[0], re.findall('model_(.*)e_lr', x)[0]))\n",
        "available_models_df = pd.DataFrame(d, columns = ('model_name', 'outcome', 'type_of_image', 'model_arch', 'number_epochs'))\n",
        "display(available_models_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uelaxvbIUkZf"
      },
      "outputs": [],
      "source": [
        "#########################################################################################################\n",
        "############### No need to run this cell if your session was not disconnected/terminated  ###############\n",
        "#########################################################################################################\n",
        "\n",
        "print('The model that you trained should have 10 for number_epochs')\n",
        "selected_model = int(input('Which model results do you want to use for model evaluation? Inidcate the row index, which starts at 0 (i.e., number) '))\n",
        "print('You selected the ' + available_models_df.loc[selected_model, 'model_name'] + ' model')\n",
        "\n",
        "# this sets the values\n",
        "my_y_col, my_pred_col, my_type_of_images, my_model_architecture, my_number_of_epochs = available_models_df.loc[selected_model, 'outcome'], 'ufp_pred', available_models_df.loc[selected_model, 'type_of_image'], available_models_df.loc[selected_model, 'model_arch'], available_models_df.loc[selected_model, 'number_epochs']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1EKFhnJOgTX"
      },
      "source": [
        "**Model Log**\n",
        "\n",
        "Before we dig into predictions, we can take a look at the model log to see the change in performance in the training and validation sets during training. Perfromance in the training set is usally higher that in the validation set, though we hope that they end up being pretty close. If they are not close, then that is an indication that the model is overfit. You would also like to see the lines in the plot flattening out in later epochs. If the lines are still very wiggly, then the models could probably benefit from more training epochs.  \n",
        "\n",
        "If you are training on categorical quintiles, the y axis will be accuracy. Randomly guessing quintiles would results in an accuracy of 0.2,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IIQdXQcTM9j"
      },
      "outputs": [],
      "source": [
        "# read in the model log\n",
        "train_log = pd.read_csv('/content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model_log/' + my_y_col + '_' + my_type_of_images + '_' + my_model_architecture + '_model_logger_'+ my_number_of_epochs+'e_lr10e-3.csv')\n",
        "\n",
        "print('Looking at ' + my_y_col + '_' + my_type_of_images + '_' + my_model_architecture + '_model_logger_'+ my_number_of_epochs+'e_lr10e-3')\n",
        "\n",
        "# if we are training on categorical, then we want to look at accuracy.\n",
        "if 'cat' in my_y_col:\n",
        "  plt.plot(train_log['epoch'], train_log['val_accuracy'])\n",
        "  plt.plot(train_log['epoch'], train_log['accuracy'])\n",
        "  plt.legend([\"validation\", \"train\"], loc=\"upper left\")\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylim(ymin = 0, ymax = 1)\n",
        "else:  # if we are training on continuous or numeric quintile, then we want to look at RMSE.\n",
        "  plt.plot(train_log['epoch'], train_log['val_rmse'])\n",
        "  plt.plot(train_log['epoch'], train_log['rmse'])\n",
        "  plt.legend([\"validation\", \"train\"], loc=\"upper left\")\n",
        "  plt.ylabel('RMSE')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylim(ymin = 0, ymax = 100000) # might need to set a limit to make it interpretable, sometimes the RMSE can start very very high\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcwszNz81U4o"
      },
      "source": [
        "**Comparing Observed to Predicted**\n",
        "\n",
        "Read in the results dataframe from the csv you saved.\n",
        "\n",
        "Recall that we trained on two images at each road segment (either zoom 18 and 19 or angles 1 and 2) and then generated predictions on two images at each road segment. You should take the average of the two predictions as the predicted value for a road segment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmzSp96l1WjB"
      },
      "outputs": [],
      "source": [
        "# read in the predictions\n",
        "results = pd.read_csv('/content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model_predictions/' + my_y_col + '_' + my_type_of_images + '_' + my_model_architecture + '_model_predictions_'+ my_number_of_epochs+'e_lr10e-3.csv')\n",
        "\n",
        "# group them by site id and take the mean prediction\n",
        "results_gr = results.groupby(['id', 'long', 'lat', 'set', 'ufp', 'bc', 'temp_f', 'relhum_pct', 'windspeed_kts'])[my_pred_col].mean().reset_index()\n",
        "\n",
        "# create a dataframe that is just the test set.\n",
        "results_gr_test = results_gr.loc[results_gr['set'] == 'test', ]\n",
        "results_gr_val = results_gr.loc[results_gr['set'] == 'validate', ]\n",
        "results_gr_train = results_gr.loc[results_gr['set'] == 'train', ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvTRiZWaPYW9"
      },
      "source": [
        "We can compare observed to predicted in a scatter plot. On the left is all the data and on the right is just in the test set. Model evaluation should only be done with the independent test set, but it can be interesting to look at all the data as well.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtR8PRXhkgMA"
      },
      "outputs": [],
      "source": [
        "if 'ufp' in my_y_col:\n",
        "  obs_col = 'ufp'\n",
        "else:\n",
        "  obs_col = 'bc'\n",
        "\n",
        "f = plt.figure(figsize=(12,5))\n",
        "ax = f.add_subplot(121)\n",
        "ax2 = f.add_subplot(122)\n",
        "ax.plot(results_gr[my_pred_col], results_gr[obs_col], 'bo')\n",
        "ax2.plot(results_gr_test[my_pred_col], results_gr_test[obs_col], 'bo')\n",
        "\n",
        "ax.set_xlabel('Predicted ' + obs_col + ' Quintile') # upadate label to quint vs continous\n",
        "ax2.set_xlabel('Predicted ' + obs_col + ' Quintile')\n",
        "ax.set_ylabel('Observed ' + obs_col + ' Concentration')\n",
        "\n",
        "ax.set_title('Predicting ' + my_y_col + ' in All Data (' + my_model_architecture + ')')\n",
        "ax2.set_title('Predicting ' + my_y_col + ' in Test Set (' + my_model_architecture + ')')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhxVYv_m1vkk"
      },
      "source": [
        "It's hard to pass or fail the eyeball test, so lets compare the predictions to the observed values in a regression. That allows us to state what proportion of the observed spatial variation in UFP concentrations in the test set are explained by the model. Here is where we include meterological conditions (i.e., weather) in order to adjust for weather-related temporal variations in the observed concentrations during monitoring.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7kJ7NfT2vjI"
      },
      "outputs": [],
      "source": [
        "# first regress the predictions and meterological conditions onto observed values in the test set\n",
        "multi_x_val, y_val = results_gr_val[[my_pred_col, 'temp_f', 'relhum_pct', 'windspeed_kts']], results_gr_val[obs_col] # .array.reshape(-1,1) for the x is there is only a single predictor\n",
        "\n",
        "# this gives us our temporal adjustment model in the validation set\n",
        "results_linear_model_val = LinearRegression().fit(multi_x_val, y_val)\n",
        "\n",
        "print(f\"Temporal adjustment slopes in validation set are: {results_linear_model_val.coef_}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IsBnNNl72ak"
      },
      "outputs": [],
      "source": [
        "# then use the temporal adjusment model on the test set data\n",
        "multi_x_test, y_test = results_gr_test[[my_pred_col, 'temp_f', 'relhum_pct', 'windspeed_kts']], results_gr_test[obs_col] # .array.reshape(-1,1) for the x is there is only a single predictor\n",
        "\n",
        "results_gr_test = results_gr_test.assign(temp_adj_pred = results_linear_model_val.predict(multi_x_test))\n",
        "\n",
        "x_test = results_gr_test['temp_adj_pred'].array.reshape(-1,1)\n",
        "\n",
        "results_linear_model_test = LinearRegression().fit(x_test, y_test)\n",
        "r_sq = results_linear_model_test.score(x_test, y_test)\n",
        "\n",
        "y_test_pred = results_linear_model_test.predict(x_test)\n",
        "mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "print(f\"MSE in test set is: {mse}\")\n",
        "print(f\"R2 in test set is: {r_sq}\")\n",
        "print(f\"Test set regression slopes is: {results_linear_model_test.coef_}\")\n",
        "print(f\"Test set intercept is: {results_linear_model_test.intercept_}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG9mR7TD2oXA"
      },
      "source": [
        "You can record the MSE, R2, and your model details here:\n",
        "https://docs.google.com/spreadsheets/d/18SN3iT4L6ixL7FUVpRNr3khos51TUM4GcS-4T-zvu-c/edit?usp=sharing\n",
        "\n",
        "If we wanted to cheat, we could do the temporal adjustment using all the data and evaluate the predictions against all the observed values (i.e., not use an independent test set). Does the R2 look better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG9cRRvn2oJQ"
      },
      "outputs": [],
      "source": [
        "x, multi_x, y = results_gr[my_pred_col].array.reshape(-1,1), results_gr[[my_pred_col, 'temp_f', 'relhum_pct', 'windspeed_kts']], results_gr[obs_col] # .array.reshape(-1,1) for the x is there is only a single predictor\n",
        "\n",
        "results_linear_model = LinearRegression().fit(multi_x, y)\n",
        "\n",
        "r_sq = results_linear_model.score(multi_x, y)\n",
        "\n",
        "print(f\"R2 in all data is: {r_sq}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLg8mP6c1sny"
      },
      "outputs": [],
      "source": [
        "# now use the temporal adjustment model to generate predictions in all the data.\n",
        "# This is so we can visualize the prediction. It isn't to evaluate model performance.\n",
        "results_gr = results_gr.assign(temp_adj_pred = results_linear_model_val.predict(multi_x))\n",
        "results_gr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv1bEM9urT6j"
      },
      "source": [
        "# Plot results\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEgzP1Og66Ww"
      },
      "outputs": [],
      "source": [
        "# we can see how using a linear regression temporal adjustment of categorical values results in continuous values\n",
        "plt.plot(results_gr['temp_adj_pred'], results_gr[obs_col], 'bo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzxeIwsKaelE"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, sharey=True, figsize=(20, 10))\n",
        "results_gr['ufp_pred_level'] = pd.qcut(results_gr['temp_adj_pred'], 9)\n",
        "sns.scatterplot(x='long', y='lat', hue= 'ufp_pred_level', palette = sns.color_palette(\"RdYlGn_r\", 9), data= results_gr, s = 8, ax = ax1).set(title='Predicted UFP')\n",
        "\n",
        "results_gr['ufp_level'] = pd.qcut(results_gr['ufp'], 9)\n",
        "sns.scatterplot(x='long', y='lat', hue= 'ufp_level', palette = sns.color_palette(\"RdYlGn_r\", 9), data= results_gr, s = 8, ax = ax2).set(title='Observed UFP')\n",
        "\n",
        "results_gr['residuals'] = results_gr['ufp'] - results_gr['temp_adj_pred']\n",
        "\n",
        "res_mean = np.mean(results_gr['residuals'])\n",
        "res_std = np.std(results_gr['residuals'])\n",
        "\n",
        "results_gr['residuals_norm'] = pd.qcut((results_gr['residuals'] - res_mean)/res_std, 9)\n",
        "\n",
        "sns.scatterplot(x='long', y='lat', data= results_gr, hue= 'residuals_norm', palette = sns.color_palette(\"vlag\", 9), s = 8, ax = ax3).set(title='Normalized Residuals')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkysjNy6e5BQ"
      },
      "source": [
        "# Class Activation Maps\n",
        "\n",
        "What part of the images are important? We can't say for sure, but we can look at how the model responds to individual images. These are \"Class Activation Maps\" (CAMs). They show what part of the image is contributing to the class prediction. It is important to note that this is for \"class\" predictions, i.e., categorical predictions. We can only make CAMs for models that generate categorical predictions. We cannot make CAMs for models that generate continous predictions.  \n",
        "\n",
        "The code below is made to run on its own without any of the inputs above. This lets you come back to this file to explore images without having to worry about any of the code above. You also have the option to use one of the pre-trained models that you moved into your Google Drive using the code just before the \"Model Evaluation\" section.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03PIwxGIRfar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a36683c-1dfe-4581-acc5-64947832db35"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e62979a40181>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mavailable_models\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ufp_quint_cat|ufp_quint_cont|ufp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sat|str'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xcep|resn|dens'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_(.*)e_lr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mavailable_models_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'model_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'outcome'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'type_of_image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_arch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'number_epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavailable_models_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
          ]
        }
      ],
      "source": [
        "# get a list the available models, this will be a mix of pre-trained models and models you have trained today\n",
        "available_models = os.listdir('/content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model/')\n",
        "\n",
        "# extract information from the model names in order to help undestand what the model was trained on\n",
        "# it's imporant to note the \"outcome\" column, you can only select a model that predicts \"cat\" for CAMs\n",
        "d = []\n",
        "for x in available_models:\n",
        "  d.append((x, re.findall('ufp_quint_cat|ufp_quint_cont|ufp', x)[0], re.findall('sat|str', x)[0], re.findall('xcep|resn|dens', x)[0], re.findall('model_(.*)e_lr', x)[0]))\n",
        "available_models_df = pd.DataFrame(d, columns = ('model_name', 'outcome', 'type_of_image', 'model_arch', 'number_epochs'))\n",
        "display(available_models_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GupbI5nS7oqa"
      },
      "source": [
        "> ***Loading a model in the next code cell takes about 15 seconds. Don't forget to enter the row number when prompted.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvYEZ7V_T8BP"
      },
      "outputs": [],
      "source": [
        "# run this code to pick and load a model\n",
        "selected_model = int(input('Which model do you want to use for Class Activation Maps? Inidcate the row index (i.e, number) '))\n",
        "print('You selected the ' + available_models_df.loc[selected_model, 'model_name'] + ' model')\n",
        "if bool(re.search('cat', available_models_df.loc[selected_model, 'outcome'])) == False:\n",
        "  raise SystemError('Cannot use continuous outcome model for CAMs. Re-run this cell and select a model that predicts categorical outcomes!')\n",
        "\n",
        "# this loads the model\n",
        "model = K.models.load_model('/content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model/' + available_models_df.loc[selected_model, 'model_name'], custom_objects={'rmse': rmse})\n",
        "\n",
        "# this sets the values\n",
        "my_y_col, my_pred_col, my_type_of_images, my_model_architecture, my_number_of_epochs = available_models_df.loc[selected_model, 'outcome'], 'ufp_pred', available_models_df.loc[selected_model, 'type_of_image'], available_models_df.loc[selected_model, 'model_arch'], available_models_df.loc[selected_model, 'number_epochs']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17x3Xosja4LF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# print out a summary, you want the name of the last layer before the activation function\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MUtTLR_yY9B"
      },
      "source": [
        "Some useful exaples of what we will do:  \n",
        "https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82\n",
        "  \n",
        " https://www.kaggle.com/code/lov4jin/densenet-imagenet-pretrained-vs-retrained-gradcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nildJi-La32w"
      },
      "outputs": [],
      "source": [
        "# define model builder, pre-processing and decoding\n",
        "# this is specific to the model architecture\n",
        "if 'xce' in my_model_architecture:\n",
        "  cam_model_builder = K.applications.xception.Xception\n",
        "  cam_pre_pro_fn = K.applications.xception.preprocess_input # the different architectures have different preprocessing functions\n",
        "  cam_decode_fn = K.applications.xception.decode_predictions\n",
        "  last_conv_layer_name = 'block14_sepconv2_act'  # you can see where this layer is in the model.summary() output\n",
        "else:\n",
        "  if 'res' in my_model_architecture:\n",
        "    cam_model_builder = K.applications.resnet_v2.ResNet50V2\n",
        "    cam_pre_pro_fn = K.applications.resnet_v2.preprocess_input\n",
        "    cam_decode_fn = K.applications.resnet_v2.decode_predictions\n",
        "    last_conv_layer_name = 'block14_sepconv2_act' # you can see where this layer is in the model.summary() output\n",
        "  else:\n",
        "    if 'den' in my_model_architecture:\n",
        "      cam_model_builder = K.applications.densenet.DenseNet121\n",
        "      cam_pre_pro_fn = K.applications.densenet.preprocess_input\n",
        "      cam_decode_fn = K.applications.densenet.decode_predictions\n",
        "      last_conv_layer_name = 'block14_sepconv2_act' # you can see where this layer is in the model.summary() output\n",
        "\n",
        "model_builder = cam_model_builder\n",
        "img_size = (240, 240)\n",
        "preprocess_input = cam_pre_pro_fn\n",
        "decode_predictions = cam_decode_fn\n",
        "\n",
        "# take a look at an image we will be working with\n",
        "if my_type_of_images == 'str':\n",
        "  first_image = 'angle1'\n",
        "  second_image = 'angle2'\n",
        "else:\n",
        "  first_image = '18'\n",
        "  second_image = '19'\n",
        "\n",
        "# this will look at image number 100. Further down we have streamlined the code so you can easily pick different images.\n",
        "img_path = '/content/images/images_' + first_image +'/00100.jpg'\n",
        "display(Image(img_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiwRssM1bSFQ"
      },
      "outputs": [],
      "source": [
        "# read in an image and turn it into an array\n",
        "def get_img_array(img_path, size):\n",
        "    img = K.preprocessing.image.load_img(img_path, target_size=size)\n",
        "    array = K.preprocessing.image.img_to_array(img)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "# take the array of the single image and give it to the model to generate a prediction\n",
        "# instead having the prediction as the output, take a look at how the last layer responds\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    # First, we create a model that maps the input image to the activations\n",
        "    # of the last conv layer as well as the output predictions\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    # Then, we compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last conv layer\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(preds[0])\n",
        "        class_channel = preds[:, pred_index]\n",
        "\n",
        "    # This is the gradient of the output neuron (top predicted or chosen)\n",
        "    # with regard to the output feature map of the last conv layer\n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    # then sum all the channels to obtain the heatmap class activation\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vb-4SC3bSCx"
      },
      "outputs": [],
      "source": [
        "# let's see how those last two function work\n",
        "# Prepare image\n",
        "img_array = preprocess_input(get_img_array(img_path, size=img_size))\n",
        "\n",
        "# Remove last layer's softmax\n",
        "model.layers[-1].activation = None\n",
        "\n",
        "# Print what the top predicted class is\n",
        "preds = model.predict(img_array)\n",
        "\n",
        "# Generate class activation heatmap\n",
        "heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
        "\n",
        "# Display heatmap\n",
        "plt.matshow(heatmap)\n",
        "plt.title('Class Probs = ' + ''.join(str(x) for x in preds))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6CSQCxgbSAD"
      },
      "outputs": [],
      "source": [
        "# now we want to overlay that heatmap onto the image that was used to generate it\n",
        "def save_and_display_gradcam(read_img_path, heatmap, cam_name = 'grad-cam-image.jpg', cam_path='/content/drive/MyDrive/colab_files/sharp_d2_UFP/output/grad_cam_images/', alpha=0.6):\n",
        "    # Load the original image\n",
        "    img = K.preprocessing.image.load_img(read_img_path)\n",
        "    img = K.preprocessing.image.img_to_array(img)\n",
        "\n",
        "    # Rescale heatmap to a range 0-255\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    # Use jet colormap to colorize heatmap\n",
        "    jet = cm.get_cmap(\"jet\")\n",
        "\n",
        "    # Use RGB values of the colormap\n",
        "    jet_colors = jet(np.arange(256))[:, :3]\n",
        "    jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "    # Create an image with RGB colorized heatmap\n",
        "    jet_heatmap = K.preprocessing.image.array_to_img(jet_heatmap)\n",
        "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "    jet_heatmap = K.preprocessing.image.img_to_array(jet_heatmap)\n",
        "\n",
        "    # Superimpose the heatmap on original image\n",
        "    superimposed_img = jet_heatmap * alpha + img\n",
        "    superimposed_img = K.preprocessing.image.array_to_img(superimposed_img)\n",
        "\n",
        "    # Save the superimposed image\n",
        "    cam_path_and_name = cam_path + cam_name\n",
        "    superimposed_img.save(cam_path_and_name)\n",
        "\n",
        "    # Display Grad CAM\n",
        "    display(Image(cam_path_and_name)) # maybe take this out of the function in order to display multiple in one plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqOI3agHg215"
      },
      "outputs": [],
      "source": [
        "# now save and display the image. give the image a meaningful name. here we name the pollutant, the type of image, and the image number\n",
        "save_and_display_gradcam(img_path, heatmap, cam_name = 'grad_cam_ufp_zoom_resn_image0001.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9tbFDUmncfs"
      },
      "source": [
        "# Streamlined CAM Code\n",
        "\n",
        "The code cell below has all the code to generate, save, and display a CAMs for each image at a given site. It is the same code from the cell above, but all in one cell to make it easier to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xi7tlAmlfZ4"
      },
      "outputs": [],
      "source": [
        "image_number = input('Image number (max 5916): ')\n",
        "\n",
        "image_number = f\"{int(image_number):05}\"\n",
        "\n",
        "if my_type_of_images == 'str':\n",
        "  first_image = 'angle1'\n",
        "  second_image = 'angle2'\n",
        "else:\n",
        "  first_image = '18'\n",
        "  second_image = '19'\n",
        "\n",
        "first_img_path = '/content/images/images_' + first_image +'/'+ image_number +'.jpg'\n",
        "second_img_path = '/content/images/images_' + second_image +'/'+ image_number +'.jpg'\n",
        "\n",
        "first_img_array = preprocess_input(get_img_array(first_img_path, size=img_size))\n",
        "model.layers[-1].activation = None\n",
        "first_preds = model.predict(first_img_array)\n",
        "first_heatmap = make_gradcam_heatmap(first_img_array, model, last_conv_layer_name)\n",
        "\n",
        "second_img_array = preprocess_input(get_img_array(second_img_path, size=img_size))\n",
        "model.layers[-1].activation = None\n",
        "second_preds = model.predict(second_img_array)\n",
        "second_heatmap = make_gradcam_heatmap(second_img_array, model, last_conv_layer_name)\n",
        "\n",
        "save_and_display_gradcam(first_img_path, first_heatmap, cam_name = 'grad_cam_' + my_model_architecture +'_' + my_y_col + '_' + first_image + 'image_'+ image_number +'.jpg')\n",
        "save_and_display_gradcam(second_img_path, second_heatmap, cam_name = 'grad_cam_' + my_model_architecture +'_' + my_y_col + '_' + second_image + 'image_'+ image_number +'.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkRtz0eNdT7u"
      },
      "source": [
        "For refence, we can also take a look at the heatmaps on their own. The plot titles include the class predictions (we take the max as the final prediciton).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ5-I7lQrziv"
      },
      "outputs": [],
      "source": [
        "img_array = preprocess_input(get_img_array(first_img_path, size=img_size))\n",
        "preds = model.predict(img_array)\n",
        "plt.matshow(first_heatmap)\n",
        "plt.title('Class Probs = ' + ''.join(str(x) for x in preds))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScBoh8G9r45h"
      },
      "outputs": [],
      "source": [
        "img_array = preprocess_input(get_img_array(second_img_path, size=img_size))\n",
        "preds = model.predict(img_array)\n",
        "plt.matshow(second_heatmap)\n",
        "plt.title('Class Probs = ' + ''.join(str(x) for x in preds))\n",
        "plt.title('Class Probs = ' + ''.join(str(x) for x in preds))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDZDBqhS77GZ"
      },
      "source": [
        "# Visualizing Filter Layers  \n",
        "\n",
        "Class Activation Maps showed us what parts of the image were important for the final class probabilities. We cannot make Class Activation maps for models trained on continuous outcomes because there are no \"classes\". Instead we can visualize the model layers when the model is generating a prediction on a image. This can help us get a feel for what features in that image the CNN model may be using to generate the prediction.  We can visualize any layer, but the deeper we go into the model, the more transformed the data is and the less interpretable it will be. Visualizing layers may not always provide clear information on what is going correctly with a model, but they can be helpful when the model is genereating strange predictions. It can provide clues when troubleshooting a model that does not meet your expectations.      \n",
        "\n",
        "\n",
        "https://towardsdatascience.com/convolutional-neural-network-feature-map-and-filter-visualization-f75012a5a49c\n",
        "\n",
        "https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPPct8cJdi9i"
      },
      "outputs": [],
      "source": [
        "# get a list the available models, this will be a mix of pre-trained models and models you have trained today\n",
        "available_models = os.listdir('/content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model/')\n",
        "\n",
        "# extract information from the model names in order to help undestand what the model was trained on\n",
        "# it's imporant to note the \"outcome\" column, you can only select a model that predicts \"cat\" for CAMs\n",
        "d = []\n",
        "for x in available_models:\n",
        "  d.append((x, re.findall('ufp_quint_cat|ufp_quint_cont|ufp', x)[0], re.findall('sat|str', x)[0], re.findall('xcep|resn|dens', x)[0], re.findall('model_(.*)e_lr', x)[0]))\n",
        "available_models_df = pd.DataFrame(d, columns = ('model_name', 'outcome', 'type_of_image', 'model_arch', 'number_epochs'))\n",
        "display(available_models_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cny-EYUsdjzl"
      },
      "outputs": [],
      "source": [
        "# run this code to pick and load a model\n",
        "selected_model = int(input('Which model do you want to use for Class Activation Maps? Inidcate the row index (i.e., number) '))\n",
        "print('You selected the ' + available_models_df.loc[selected_model, 'model_name'] + ' model')\n",
        "\n",
        "# this loads the model\n",
        "model = K.models.load_model('/content/drive/MyDrive/colab_files/sharp_d2_UFP/output/model/' + available_models_df.loc[selected_model, 'model_name'], custom_objects={'rmse': rmse})\n",
        "\n",
        "# this sets the values\n",
        "my_y_col, my_pred_col, my_type_of_images, my_model_architecture, my_number_of_epochs = available_models_df.loc[selected_model, 'outcome'], 'ufp_pred', available_models_df.loc[selected_model, 'type_of_image'], available_models_df.loc[selected_model, 'model_arch'], available_models_df.loc[selected_model, 'number_epochs']\n",
        "\n",
        "if my_model_architecture == 'xcep':\n",
        "  my_pre_pro_fn = K.applications.xception.preprocess_input\n",
        "else:\n",
        "  if my_model_architecture == 'resn':\n",
        "    my_pre_pro_fn = K.applications.resnet_v2.preprocess_input\n",
        "  else:\n",
        "    if my_model_architecture == 'dens':\n",
        "      my_pre_pro_fn = K.applications.densenet.preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufJtfnOH8x9I"
      },
      "outputs": [],
      "source": [
        "# in case we need to reload the metadata\n",
        "metadata = pd.read_csv(\"/content/drive/MyDrive/sharp_ml_course/pre_trained_models/ufp_colombia_metadata.csv\")\n",
        "if my_type_of_images == 'sat':\n",
        "  metadata['train_file'] = metadata['sat_file']\n",
        "else:\n",
        "  metadata['train_file'] = metadata['str_file']\n",
        "\n",
        "my_batch_size = 32\n",
        "image_dir = '/content/images/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrGqNdhj76xG"
      },
      "outputs": [],
      "source": [
        "img_size = (240, 240)\n",
        "preprocess_input = my_pre_pro_fn\n",
        "img_path = '/content/images/images_angle1/01000.jpg' # 100 is a swimming pool\n",
        "\n",
        "def get_img_array(img_path, size):\n",
        "    img = K.preprocessing.image.load_img(img_path, target_size=size)\n",
        "    array = K.preprocessing.image.img_to_array(img)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "display(Image(img_path))\n",
        "img_array = preprocess_input(get_img_array(img_path, size=img_size))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssTR9q-28Le5"
      },
      "outputs": [],
      "source": [
        "# visualizing every channel in every intermediate activation (or the first 6)\n",
        "\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
        "activations = activation_model.predict(img_array)\n",
        "\n",
        "# only looking at first 6 layers, more than that can get confusing, plus layers after have 256 channels....which is a lot of visualize\n",
        "sub_activations = activations[0:6]\n",
        "\n",
        "layer_names = []\n",
        "for layer in model.layers[0:16]:\n",
        "    layer_names.append(layer.name) # Names of the layers, so you can have them as part of your plot\n",
        "\n",
        "images_per_row = 16\n",
        "\n",
        "for layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n",
        "    n_features = layer_activation.shape[-1] # Number of features in the feature map\n",
        "    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n",
        "    n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n",
        "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
        "    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n",
        "        for row in range(images_per_row):\n",
        "            channel_image = layer_activation[0,\n",
        "                                             :, :,\n",
        "                                             col * images_per_row + row]\n",
        "            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n",
        "            channel_image /= channel_image.std()\n",
        "            channel_image *= 64\n",
        "            channel_image += 128\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
        "            display_grid[col * size : (col + 1) * size, # Displays the grid\n",
        "                         row * size : (row + 1) * size] = channel_image\n",
        "    scale = 1. / size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                        scale * display_grid.shape[0]))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
        "    #plt.imsave('to_19_16028868_'+layer_name+'.png', display_grid, cmap='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV-3E34Y21cv"
      },
      "source": [
        "Keep in mind that visualizing the filter layers probably won't be able to give us specific answers about when the CNN model is doing, but it might be able to provide some clues about what part of an image might be important with respect to the prediction generated. Also keep in mind that as you go deeper in the CNN (i.e. the later layers), the image will become progressively more transformed and less interpretable. You might see some activation maps that are all zero (i.e. all dark), which can indicate a dead filter. That is not necessarily a problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjUOUJCp6FrF"
      },
      "source": [
        "# END"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}